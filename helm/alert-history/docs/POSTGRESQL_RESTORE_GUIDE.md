# PostgreSQL Disaster Recovery & Restore Guide (TN-98)

**Target**: Alertmanager++ OSS Core - Standard Profile
**Quality**: 150% (Grade A+ EXCEPTIONAL)
**Date**: 2024
**Version**: 1.0

---

## ðŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Backup Architecture](#backup-architecture)
3. [Restore Procedures](#restore-procedures)
4. [Point-in-Time Recovery (PITR)](#point-in-time-recovery-pitr)
5. [Disaster Recovery Scenarios](#disaster-recovery-scenarios)
6. [Testing & Validation](#testing--validation)
7. [Troubleshooting](#troubleshooting)
8. [Best Practices](#best-practices)

---

## 1. Overview

### Backup Strategy

Alertmanager++ PostgreSQL uses a **two-tier backup strategy**:

1. **Base Backups** (Full)
   - Frequency: Daily at 2 AM UTC (configurable via `postgresql.backup.schedule`)
   - Method: `pg_basebackup` (tar.gz format)
   - Retention: 30 days (configurable via `postgresql.backup.retention`)
   - Location: `/backup/base/YYYYMMDD_HHMMSS/`

2. **WAL Archives** (Incremental)
   - Frequency: Continuous (every 5 minutes or 16MB, whichever comes first)
   - Method: `archive_command` (copy to `/backup/wal_archive/`)
   - Retention: 7 days (configurable via `postgresql.backup.walRetention`)
   - Location: `/backup/wal_archive/`

### Recovery Capabilities

- **Full Restore**: Restore from latest base backup
- **Point-in-Time Recovery (PITR)**: Restore to any point within WAL retention window (7 days)
- **Disaster Recovery**: Complete cluster rebuild from backups

### RTO & RPO

- **RTO (Recovery Time Objective)**: < 30 minutes (for databases < 100GB)
- **RPO (Recovery Point Objective)**: < 5 minutes (WAL archive frequency)

---

## 2. Backup Architecture

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL StatefulSet                                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     WAL Files      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚  archive_command  â”‚    â”‚
â”‚  â”‚  PostgreSQL  â”‚                     â”‚  (every 5 min)    â”‚    â”‚
â”‚  â”‚   Primary    â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”‚              â”‚                              â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚                â”‚
â”‚                                                 â–¼                â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                                      â”‚  /backup/wal_archiveâ”‚    â”‚
â”‚                                      â”‚  (PVC: 50Gi)        â”‚    â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backup CronJob (Daily 2 AM UTC)                                â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  pg_basebackup   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> /backup/base/YYYYMMDD/      â”‚
â”‚  â”‚  (tar.gz format) â”‚              (Full base backup)           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backup Files Structure

```
/backup/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ 20241130_020000/    # Base backup from Nov 30
â”‚   â”‚   â”œâ”€â”€ base.tar.gz     # Database files
â”‚   â”‚   â”œâ”€â”€ pg_wal.tar.gz   # WAL files at backup time
â”‚   â”‚   â””â”€â”€ backup.log      # Backup execution log
â”‚   â”œâ”€â”€ 20241129_020000/    # Previous day backup
â”‚   â””â”€â”€ ...
â””â”€â”€ wal_archive/
    â”œâ”€â”€ 000000010000000000000001  # WAL segment 1
    â”œâ”€â”€ 000000010000000000000002  # WAL segment 2
    â””â”€â”€ ...
```

---

## 3. Restore Procedures

### Prerequisites

1. Access to backup PVC (`alert-history-postgresql-backup`)
2. kubectl access with sufficient RBAC permissions
3. PostgreSQL StatefulSet scaled down (if replacing existing database)

### Procedure 1: Full Restore from Latest Backup

**Use Case**: Complete data loss, corruption, or disaster recovery

**Steps**:

```bash
# 1. Scale down PostgreSQL StatefulSet
kubectl scale statefulset alert-history-postgresql --replicas=0 -n alert-history

# 2. Delete existing PVC (CAUTION: This deletes all data)
kubectl delete pvc postgres-data-alert-history-postgresql-0 -n alert-history

# 3. Get latest backup directory
LATEST_BACKUP=$(kubectl exec -n alert-history \
  $(kubectl get pod -n alert-history -l app.kubernetes.io/component=database-backup -o jsonpath='{.items[0].metadata.name}') \
  -- ls -t /backup/base | head -1)

echo "Latest backup: $LATEST_BACKUP"

# 4. Create restore Job
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: postgresql-restore-$(date +%s)
  namespace: alert-history
spec:
  template:
    spec:
      restartPolicy: Never
      securityContext:
        fsGroup: 999
        runAsNonRoot: true
      containers:
      - name: restore
        image: postgres:16
        command:
        - /bin/bash
        - -c
        - |
          set -euo pipefail

          echo "==================================="
          echo "PostgreSQL Full Restore - TN-98"
          echo "==================================="

          BACKUP_DIR="/backup/base/$LATEST_BACKUP"
          PGDATA="/var/lib/postgresql/data/pgdata"

          # Extract base backup
          echo "Extracting base backup from \$BACKUP_DIR..."
          mkdir -p "\$PGDATA"
          cd "\$PGDATA"
          tar -xzf "\$BACKUP_DIR/base.tar.gz"

          # Extract WAL backup
          echo "Extracting WAL backup..."
          mkdir -p "\$PGDATA/pg_wal"
          tar -xzf "\$BACKUP_DIR/pg_wal.tar.gz" -C "\$PGDATA/pg_wal"

          # Set ownership
          chown -R 999:999 "\$PGDATA"

          echo "Restore completed successfully!"
          echo "PostgreSQL will replay WAL on startup."
          echo "==================================="
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        - name: backup
          mountPath: /backup
          readOnly: true
        securityContext:
          runAsUser: 999
          runAsGroup: 999
      volumes:
      - name: postgres-data
        persistentVolumeClaim:
          claimName: postgres-data-alert-history-postgresql-0
      - name: backup
        persistentVolumeClaim:
          claimName: alert-history-postgresql-backup
EOF

# 5. Wait for restore Job to complete
kubectl wait --for=condition=complete --timeout=600s job/postgresql-restore-* -n alert-history

# 6. Scale up PostgreSQL StatefulSet
kubectl scale statefulset alert-history-postgresql --replicas=1 -n alert-history

# 7. Verify PostgreSQL is running
kubectl wait --for=condition=ready pod/alert-history-postgresql-0 -n alert-history --timeout=300s

# 8. Check logs
kubectl logs -f alert-history-postgresql-0 -n alert-history
```

---

## 4. Point-in-Time Recovery (PITR)

### Use Case

Restore database to a specific point in time (e.g., before accidental DELETE, corruption event).

### Prerequisites

- WAL archives available for target recovery point
- Base backup older than target recovery point

### Procedure

```bash
# 1. Determine target recovery time
TARGET_TIME="2024-11-30 14:30:00 UTC"  # Adjust to your needs

# 2. Scale down PostgreSQL
kubectl scale statefulset alert-history-postgresql --replicas=0 -n alert-history

# 3. Delete existing PVC
kubectl delete pvc postgres-data-alert-history-postgresql-0 -n alert-history

# 4. Create PITR restore Job
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: postgresql-pitr-restore-$(date +%s)
  namespace: alert-history
spec:
  template:
    spec:
      restartPolicy: Never
      securityContext:
        fsGroup: 999
        runAsNonRoot: true
      containers:
      - name: restore
        image: postgres:16
        command:
        - /bin/bash
        - -c
        - |
          set -euo pipefail

          echo "==================================="
          echo "PostgreSQL PITR Restore - TN-98"
          echo "==================================="
          echo "Target recovery time: $TARGET_TIME"

          BACKUP_DIR="/backup/base/\$(ls -t /backup/base | head -1)"
          PGDATA="/var/lib/postgresql/data/pgdata"

          # Extract base backup
          echo "Extracting base backup from \$BACKUP_DIR..."
          mkdir -p "\$PGDATA"
          cd "\$PGDATA"
          tar -xzf "\$BACKUP_DIR/base.tar.gz"
          tar -xzf "\$BACKUP_DIR/pg_wal.tar.gz" -C "\$PGDATA/pg_wal"

          # Create recovery.signal file
          touch "\$PGDATA/recovery.signal"

          # Configure recovery
          cat >> "\$PGDATA/postgresql.auto.conf" <<EOC
restore_command = 'cp /backup/wal_archive/%f %p'
recovery_target_time = '$TARGET_TIME'
recovery_target_action = 'promote'
EOC

          # Set ownership
          chown -R 999:999 "\$PGDATA"

          echo "PITR configuration complete!"
          echo "PostgreSQL will replay WAL to target time on startup."
          echo "==================================="
        env:
        - name: TARGET_TIME
          value: "$TARGET_TIME"
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        - name: backup
          mountPath: /backup
          readOnly: true
        securityContext:
          runAsUser: 999
          runAsGroup: 999
      volumes:
      - name: postgres-data
        persistentVolumeClaim:
          claimName: postgres-data-alert-history-postgresql-0
      - name: backup
        persistentVolumeClaim:
          claimName: alert-history-postgresql-backup
EOF

# 5. Wait for restore to complete
kubectl wait --for=condition=complete --timeout=600s job/postgresql-pitr-restore-* -n alert-history

# 6. Scale up PostgreSQL
kubectl scale statefulset alert-history-postgresql --replicas=1 -n alert-history

# 7. Monitor recovery logs
kubectl logs -f alert-history-postgresql-0 -n alert-history | grep -E "recovery|replay"

# 8. Verify recovery completed
kubectl exec -it alert-history-postgresql-0 -n alert-history -- psql -U alert_history -d alert_history -c "SELECT pg_is_in_recovery();"
# Should return 'f' (false) when recovery is complete
```

---

## 5. Disaster Recovery Scenarios

### Scenario 1: Pod Crash / Node Failure

**Impact**: Temporary data unavailability
**Recovery**: Automatic (Kubernetes restarts pod)
**RTO**: < 5 minutes
**RPO**: 0 (no data loss, using persistent volume)

**No manual action required** - Kubernetes will automatically reschedule the pod.

---

### Scenario 2: PVC Corruption / Data Loss

**Impact**: Complete database loss
**Recovery**: Full restore from backup
**RTO**: < 30 minutes
**RPO**: < 5 minutes (last WAL archive)

**Action**: Follow [Procedure 1: Full Restore](#procedure-1-full-restore-from-latest-backup)

---

### Scenario 3: Accidental Data Deletion

**Impact**: Partial data loss (specific tables/rows)
**Recovery**: Point-in-Time Recovery (PITR)
**RTO**: < 30 minutes
**RPO**: 0 (exact point before deletion)

**Action**: Follow [PITR Procedure](#4-point-in-time-recovery-pitr)

---

### Scenario 4: Cluster-Wide Disaster

**Impact**: Complete infrastructure loss
**Recovery**: Rebuild cluster + restore from offsite backups
**RTO**: < 4 hours
**RPO**: < 24 hours (last offsite backup)

**Prerequisites**: Offsite backup copies (S3, GCS, Azure Blob, etc.)

**Action**:
1. Rebuild Kubernetes cluster
2. Deploy Helm chart (`helm install alert-history`)
3. Copy backups from offsite storage to backup PVC
4. Follow [Full Restore procedure](#procedure-1-full-restore-from-latest-backup)

---

## 6. Testing & Validation

### Regular DR Drills

**Frequency**: Quarterly (every 3 months)

**Procedure**:
1. Create test namespace: `kubectl create ns alert-history-dr-test`
2. Deploy Helm chart to test namespace
3. Perform full restore from production backups
4. Validate data integrity (row counts, checksums)
5. Test application connectivity
6. Document results and RTO/RPO achieved
7. Cleanup: `kubectl delete ns alert-history-dr-test`

### Backup Validation Script

```bash
#!/bin/bash
# validate-backups.sh - TN-98

set -euo pipefail

echo "PostgreSQL Backup Validation - TN-98"
echo "====================================="

# Check last base backup age
LAST_BACKUP=$(kubectl exec -n alert-history alert-history-postgresql-backup-* -- ls -t /backup/base | head -1)
LAST_BACKUP_AGE=$(kubectl exec -n alert-history alert-history-postgresql-backup-* -- stat -c %Y /backup/base/$LAST_BACKUP)
CURRENT_TIME=$(date +%s)
AGE_HOURS=$(( (CURRENT_TIME - LAST_BACKUP_AGE) / 3600 ))

echo "Last base backup: $LAST_BACKUP ($AGE_HOURS hours ago)"

if [ $AGE_HOURS -gt 48 ]; then
  echo "âš ï¸  WARNING: Last backup is older than 48 hours!"
  exit 1
fi

# Check WAL archive age
LAST_WAL=$(kubectl exec -n alert-history alert-history-postgresql-backup-* -- ls -t /backup/wal_archive | head -1)
LAST_WAL_AGE=$(kubectl exec -n alert-history alert-history-postgresql-backup-* -- stat -c %Y /backup/wal_archive/$LAST_WAL)
WAL_AGE_MINUTES=$(( (CURRENT_TIME - LAST_WAL_AGE) / 60 ))

echo "Last WAL archive: $LAST_WAL ($WAL_AGE_MINUTES minutes ago)"

if [ $WAL_AGE_MINUTES -gt 30 ]; then
  echo "âš ï¸  WARNING: Last WAL archive is older than 30 minutes!"
  exit 1
fi

# Check backup size
BACKUP_SIZE=$(kubectl exec -n alert-history alert-history-postgresql-backup-* -- du -sh /backup/base/$LAST_BACKUP | cut -f1)
echo "Backup size: $BACKUP_SIZE"

# Check disk usage
DISK_USAGE=$(kubectl exec -n alert-history alert-history-postgresql-backup-* -- df -h /backup | tail -1 | awk '{print $5}')
echo "Backup disk usage: $DISK_USAGE"

if [[ ${DISK_USAGE%?} -gt 80 ]]; then
  echo "âš ï¸  WARNING: Backup disk usage is above 80%!"
  exit 1
fi

echo "âœ… All backup validations passed!"
```

---

## 7. Troubleshooting

### Issue 1: Backup Job Fails

**Symptoms**: CronJob shows failed status, no new backups

**Diagnosis**:
```bash
kubectl logs -n alert-history job/alert-history-postgresql-backup-<timestamp>
```

**Common Causes**:
- Insufficient disk space on backup PVC
- PostgreSQL connection timeout
- Permissions issues (fsGroup=999)

**Solution**:
1. Check backup PVC size: `kubectl describe pvc alert-history-postgresql-backup -n alert-history`
2. Increase storage: `kubectl edit pvc alert-history-postgresql-backup -n alert-history`
3. Verify PostgreSQL connectivity: `kubectl exec -it alert-history-postgresql-0 -n alert-history -- pg_isready`

---

### Issue 2: WAL Archiving Stopped

**Symptoms**: No new files in `/backup/wal_archive/`, replication lag increasing

**Diagnosis**:
```bash
kubectl exec -it alert-history-postgresql-0 -n alert-history -- psql -U alert_history -c "SELECT * FROM pg_stat_archiver;"
```

**Common Causes**:
- Backup PVC full
- archive_command failure (test -f check failing)

**Solution**:
```bash
# Check backup PVC space
kubectl exec -it alert-history-postgresql-0 -n alert-history -- df -h /backup

# Manually test archive command
kubectl exec -it alert-history-postgresql-0 -n alert-history -- bash -c "test ! -f /backup/wal_archive/test && echo OK"
```

---

### Issue 3: Restore Fails with "recovery_target_time not reached"

**Symptoms**: PostgreSQL refuses to start after PITR restore

**Diagnosis**: Target recovery time is beyond available WAL archives

**Solution**:
1. Check available WAL range: `ls -lh /backup/wal_archive/`
2. Adjust `recovery_target_time` to earlier point
3. Or use `recovery_target_action = 'shutdown'` for manual inspection

---

## 8. Best Practices

### Backup Hygiene

1. **Test restores regularly** (quarterly DR drills)
2. **Monitor backup job execution** (Prometheus alerts)
3. **Validate backup integrity** (checksums, test restores)
4. **Keep backups offsite** (S3, GCS, Azure Blob)
5. **Document procedures** (update this guide as needed)

### Retention Policies

- **Base backups**: 30 days (adjust based on compliance requirements)
- **WAL archives**: 7 days (sufficient for PITR window)
- **Offsite backups**: 1 year (for long-term recovery)

### Security

- **Encrypt backups at rest** (StorageClass with encryption)
- **Encrypt backups in transit** (S3 with TLS, scp with SSH)
- **Restrict access to backups** (RBAC, IAM roles)
- **Audit backup access** (CloudTrail, audit logs)

### Automation

- **Backup validation script** (run daily via CronJob)
- **Prometheus alerts** (backup age, WAL archive age, disk usage)
- **Slack notifications** (backup success/failure)
- **Grafana dashboard** (backup metrics visualization)

---

## ðŸ“ž Support

For issues or questions:
- Slack: `#alertmanager-plus-plus`
- Email: `sre@example.com`
- Runbook: `/docs/runbooks/postgresql-disaster-recovery.md`

---

**Document Version**: 1.0
**Last Updated**: 2024
**Owner**: SRE Team
**Related**: TN-98 (PostgreSQL StatefulSet), TN-97 (HPA Cluster Mode)
