# ===============================
# Application Configuration
# ===============================
replicaCount: 1
environment: production
logLevel: info
namespace: ""

# ===============================
# Deployment Profile (TN-96)
# ===============================
# Profile determines the deployment architecture:
# - "lite": Single-node, embedded storage (SQLite), memory-only cache, zero external dependencies
# - "standard": HA-ready, PostgreSQL + Valkey/Redis, distributed cache, 2-10 replicas
profile: "standard"  # Options: "lite" | "standard"

# ===============================
# Container Image Configuration
# ===============================
image:
  repository: ipiton/alert-history-llm
  tag: "1.1.9"
  pullPolicy: IfNotPresent

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# ===============================
# Service Account Configuration
# ===============================
serviceAccount:
  create: true
  annotations: {}
  name: ""
  # RBAC for secrets management
  rbac:
    create: true
    # Allow cross-namespace target discovery
    crossNamespace: false

# ===============================
# Pod Security and Resource Configuration
# ===============================
podAnnotations: {}
podLabels: {}

podSecurityContext:
  fsGroup: 65534
  runAsNonRoot: true
  runAsUser: 65534

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 65534

resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 512Mi

# ===============================
# Service Configuration (Load Balancing)
# ===============================
service:
  type: ClusterIP
  port: 8080
  metricsPort: 9090
  annotations: {}
  # Load balancing configuration
  loadBalancerPolicy: false  # Set to true for session affinity
  sessionAffinityTimeout: 10800  # 3 hours

# ===============================
# Ingress Configuration
# ===============================
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: alert-history.local
      paths:
        - path: /
          pathType: Prefix
  tls: []
  #  - secretName: alert-history-tls
  #    hosts:
  #      - alert-history.local

# ===============================
# Health Probes Configuration (12-Factor App)
# ===============================
probes:
  liveness:
    path: /healthz  # Kubernetes liveness probe (12-Factor compliance)
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  readiness:
    path: /readyz  # Kubernetes readiness probe (12-Factor compliance)
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
  startup:
    path: /healthz  # Startup probe
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30

# ===============================
# Horizontal Pod Autoscaler
# ===============================
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  customMetrics:
    enabled: true
    requestsPerSecond: "50"
    classificationQueueSize: "10"
    publishingQueueSize: "20"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      percentPolicy: 50
      podsPolicy: 2
      periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      percentPolicy: 100
      podsPolicy: 4
      periodSeconds: 30

# ===============================
# Deployment Strategy
# ===============================
strategy:
  type: RollingUpdate
  maxUnavailable: 25%
  maxSurge: 25%

# ===============================
# Graceful Shutdown Configuration (12-Factor App)
# ===============================
gracefulShutdown:
  terminationGracePeriodSeconds: 30  # Time to wait for graceful shutdown
  preStopDelay: 5  # Delay before starting shutdown to allow load balancer updates

# ===============================
# Secrets Management Configuration
# ===============================
secrets:
  # Application secrets (base64 encoded in secrets)
  jwtSecret: ""  # JWT signing secret
  encryptionKey: ""  # Data encryption key

  # External secrets (if using external secret operators)
  externalSecrets:
    enabled: false
    secretStore: ""  # External secret store name

# ===============================
# Target Discovery Configuration (disabled - using static publishers)
# ===============================
targetDiscovery:
  enabled: false
  crossNamespace: false
  namespaces: []
  labels: []
  refreshInterval: "300s"

# ===============================
# Lite Profile Configuration (TN-96)
# ===============================
# Storage for Lite profile (SQLite database)
liteProfile:
  persistence:
    enabled: true
    size: 5Gi
    storageClass: ""
    mountPath: "/data"
  # Resource limits for Lite profile
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi

# ===============================
# PostgreSQL Database Configuration (Standard Profile Only)
# ===============================
# Automatically disabled for Lite profile
postgresql:
  enabled: true  # Overridden by profile in deployment.yaml
  replicas: 1  # Single instance for Standard Profile
  # Database configuration
  database: "alert_history"
  username: "alert_history"
  password: "secure_password_123"
  # Resource limits (TN-98: increased for production)
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi  # Increased from 1Gi for max_connections=250
    requests:
      cpu: 500m
      memory: 1Gi  # Increased from 512Mi
  # Storage configuration
  persistence:
    enabled: true
    size: 10Gi
    storageClass: ""

  # Pod Disruption Budget (TN-98: HA support)
  podDisruptionBudget:
    enabled: true
    minAvailable: 1  # Keep at least 1 pod available during voluntary disruptions

  # PostgreSQL Exporter (TN-98: 150% quality - 50+ metrics)
  exporter:
    enabled: true  # Enable postgres-exporter sidecar
    image: quay.io/prometheuscommunity/postgres-exporter
    tag: "v0.15.0"  # Latest stable as of 2024
    pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 64Mi

  # PostgreSQL Backup & Disaster Recovery (TN-98: PITR capability)
  backup:
    enabled: true  # Enable WAL archiving + base backups
    schedule: "0 2 * * *"  # Daily at 2 AM UTC
    retention: 30  # Keep base backups for 30 days
    walRetention: 7  # Keep WAL archives for 7 days (sufficient for PITR)
    storage:
      size: 50Gi  # Backup storage size (adjust based on database size)
      storageClass: ""  # Use default StorageClass
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 250m
        memory: 256Mi

  # PostgreSQL Monitoring (TN-98: 150% observability)
  monitoring:
    prometheusRules:
      enabled: true  # Enable Prometheus alerting rules
    # Grafana dashboard: Import from https://grafana.com/grafana/dashboards/9628 (PostgreSQL Database)

  # PostgreSQL Network Policy (TN-98: Security hardening)
  networkPolicy:
    enabled: false  # Enable in production for network isolation

  # PostgreSQL Configuration (TN-97 + TN-98: optimized for HPA cluster mode)
  # CRITICAL: These settings prevent connection pool exhaustion at scale
  # Connection calculation: 10 replicas × 20 conns/pod + 50 reserved = 250 max_connections
  config:
    # Connection Settings (TN-97: CRITICAL for HPA)
    maxConnections: 250  # Up from 100 default (supports 10 replicas)
    superuserReservedConnections: 10

    # Memory Settings (tuned for max_connections=250)
    sharedBuffers: "256MB"  # 25% of maxConnections
    effectiveCacheSize: "1GB"  # 50% of total memory
    workMem: "4MB"  # Per-operation memory
    maintenanceWorkMem: "64MB"  # For VACUUM, indexes

    # WAL Settings
    walBuffers: "16MB"
    maxWalSize: "4GB"
    minWalSize: "1GB"
    checkpointCompletionTarget: 0.9

    # Query Planning (SSD-optimized, TN-97)
    randomPageCost: 1.1  # Down from 4.0 (HDD default)
    effectiveIoConcurrency: 200  # Up from 1 (HDD default)

    # Monitoring (TN-97: pg_stat_statements)
    sharedPreloadLibraries: "pg_stat_statements"
    trackActivities: "on"
    trackCounts: "on"
    trackIoTiming: "on"

    # Logging (adjust for debugging connection issues)
    loggingCollector: "on"
    logConnections: "off"  # Enable for debugging (performance hit)
    logDisconnections: "off"
    logMinDurationStatement: 1000  # Log queries > 1s

    # Autovacuum (TN-97: critical for high-write alert ingestion)
    autovacuum: "on"
    autovacuumMaxWorkers: 3
    autovacuumNaptime: "1min"

    # Security (TN-98: production hardening)
    ssl: "off"  # Enable in production with certs

# ===============================
# Valkey Cache Configuration (Redis-compatible, Standard Profile Only)
# ===============================
# Automatically disabled for Lite profile (uses memory-only cache)
# ===============================
# External Secrets Operator (TN-100: Production Security)
# ===============================
# Enable External Secrets Operator for production secret management
# Integrates with AWS Secrets Manager, GCP Secret Manager, Azure Key Vault, HashiCorp Vault
externalSecrets:
  enabled: false  # Set to true in production with ESO installed
  secretStore: "default"  # Name of SecretStore or ClusterSecretStore
  secretStoreKind: "SecretStore"  # SecretStore or ClusterSecretStore
  refreshInterval: "1h"  # How often to sync secrets
  keyPath: "alertmanager-plus-plus"  # Path in secret manager

cache:
  enabled: true  # Overridden by profile in deployment.yaml
  # Valkey connection details
  host: "{{ include \"alerthistory.fullname\" . }}-valkey"  # Service name of Valkey
  port: 6379
  # Valkey authentication (if needed)
  auth:
    enabled: false
    password: ""
  # Valkey resource limits
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi

# ===============================
# Valkey/Redis StatefulSet Configuration (TN-99: Production-Ready)
# ===============================
# Valkey is a Redis-compatible cache used by Standard Profile only
# Connection pool: 50 conns/pod × 10 replicas = 500 connections
# Valkey maxclients: 10,000 (default) → 5% utilization at max scale ✅
valkey:
  enabled: true  # Managed by profile in deployment

  # Deployment configuration
  replicas: 1  # Expandable to 3 for Sentinel HA in future

  # Container image (Redis or Valkey)
  image:
    repository: redis  # Options: "redis" or "valkey/valkey"
    tag: "7-alpine"
    pullPolicy: IfNotPresent

  # Resource limits (TN-99: production-sized)
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi

  # Persistence (TN-99: enabled for durability)
  storage:
    className: ""
    requestedSize: 5Gi

  # Redis/Valkey settings (production-tuned for Standard Profile + HPA)
  settings:
    maxmemory: 384mb  # 75% of 512Mi limit (leaves headroom for overhead)
    maxmemoryPolicy: allkeys-lru  # Evict least recently used keys
    appendonly: "yes"  # AOF persistence enabled (write-ahead log)
    appendfsync: everysec  # Fsync every 1s (balance performance/durability, <1s data loss)
    loglevel: notice  # Options: debug, verbose, notice, warning
    slowlogThreshold: 10000  # Log queries >10ms (microseconds)

  # redis-exporter Sidecar (Prometheus metrics)
  exporter:
    enabled: true
    image: quay.io/oliver006/redis_exporter
    tag: v1.55.0
    pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 64Mi

  # Password authentication (Secret management)
  password:
    # Use existing Secret (recommended for production)
    existingSecret: ""  # If empty, creates new Secret
    secretKey: password
    # Password value (only used if existingSecret is empty)
    # In production: Set via Helm values or External Secrets Operator (TN-100)
    value: ""  # If empty, generates random 32-char password

  # NetworkPolicy (pod isolation)
  networkPolicy:
    enabled: false  # Enable in production for security hardening

# ===============================
# Alert Processing Configuration
# ===============================
alerts:
  retentionDays: 30
  batchSize: 100
  maxConcurrentAlerts: 50
  enableClassification: true

# ===============================
# LLM Configuration
# ===============================
llm:
  enabled: true
  proxyUrl: "https://llm-proxy.b2broker.tech"
  apiKey: "sk-eEyKBRlxsrWB81yZT5Mc1w"  # Set via LLM_API_KEY environment variable or secret
  model: "openai/gpt-4o"
  timeout: 30
  maxRetries: 3
  retryDelay: 1.0
  cacheTtl: 3600
  batchSize: 10

  # LLM API Key Secret (prefer external secrets in production)
  secret:
    enabled: false
    name: "llm-api-key"
    key: "api-key"

  # External secrets configuration
  externalSecrets:
    enabled: false
    secretStore: ""
    secretName: ""
    secretKey: "api-key"

# ===============================
# Database Migration Configuration
# ===============================
migration:
  enabled: false
  autoMigrate: false
  batchSize: 1000
  verifyData: true
  backupBeforeMigration: true

# ===============================
# Monitoring Configuration
# ===============================
monitoring:
  prometheusEnabled: true
  healthCheckInterval: 30

# (duplicate targetDiscovery section removed)

# ===============================
# Alert Publishing Configuration
# ===============================
publishing:
  enabled: true
  maxConcurrent: 10
  timeout: 30
  retries: 3

# ===============================
# Rootly Integration (Priority)
# ===============================
rootly:
  enabled: false  # Set to true to enable Rootly integration

  # Rootly API configuration
  webhookUrl: ""  # Rootly webhook URL (required)
  apiKey: ""      # Rootly API key (prefer external secrets)
  authToken: ""   # Optional auth token
  orgId: ""       # Rootly organization ID

  # Custom headers for Rootly API
  customHeaders: {}
    # X-Custom-Header: "value"

  # Incident management settings
  incidentSettings:
    autoCreate: true          # Auto-create incidents from critical alerts
    severity: "high"          # Default incident severity
    assignTeam: ""            # Default team assignment
    tags: []                  # Default incident tags
      # - "kubernetes"
      # - "production"

  # Rootly-specific filtering
  filterConfig:
    severity: ["critical", "warning"]     # Only forward critical/warning alerts
    namespaces: ["production"]            # Production namespace only
    excludeNoise: true                    # Exclude noisy alerts
    minConfidence: 0.8                    # High confidence threshold
    alertNamePattern: ""                  # Optional regex pattern

    # Rootly-specific filters
    incidentTypes: ["infrastructure", "application"]  # Incident types
    services: []              # Target specific services
      # - "api-gateway"
      # - "database"
    environments: ["production"]  # Target environments

# ===============================
# Publishing Targets (Examples)
# ===============================
publishingTargets: []
# Example configuration for dynamic publishing targets:
# publishingTargets:
#   # Rootly integration
#   - name: rootly-production
#     type: webhook
#     format: rootly
#     url: https://api.rootly.com/webhooks/your-webhook-id
#     enabled: true
#     secret:
#       apiKey: "your-rootly-api-key"
#     filterConfig:
#       severity: ["critical", "warning"]
#       excludeNoise: true
#       minConfidence: 0.7
#       namespaces: ["production"]
#
#   # PagerDuty integration
#   - name: pagerduty-oncall
#     type: webhook
#     format: pagerduty
#     url: https://events.pagerduty.com/v2/enqueue
#     enabled: true
#     secret:
#       routingKey: "your-pagerduty-routing-key"
#     filterConfig:
#       severity: ["critical"]
#       excludeNoise: true
#       minConfidence: 0.8
#
#   # Slack notifications
#   - name: slack-alerts
#     type: webhook
#     format: slack
#     url: https://hooks.slack.com/your-webhook-url
#     enabled: true
#     secret:
#       token: "your-slack-token"
#     filterConfig:
#       severity: ["critical", "warning"]
#       namespaces: ["production", "staging"]
#       alertNamePattern: "^(HighCPU|HighMemory|DiskSpace).*"
#
#   # Generic webhook example
#   - name: custom-webhook
#     type: webhook
#     format: alertmanager
#     url: https://your-webhook-endpoint.com/alerts
#     enabled: false
#     secret:
#       apiKey: "your-api-key"
#       customHeaders:
#         X-Auth-Token: "bearer-token"
#         X-Source: "alert-history"
#     filterConfig:
#       severity: ["critical", "warning", "info"]
#       excludeNoise: false

# ===============================
# Filter Rules Configuration
# ===============================
filters: {}
# Example:
# filters:
#   block_test_alerts: '{"namespace_pattern": ".*test.*"}'
#   high_confidence_only: '{"llm_confidence_above": 0.8}'

# ===============================
# Persistence Configuration (Legacy SQLite)
# ===============================
persistence:
  enabled: false  # Disabled when PostgreSQL is used
  accessMode: ReadWriteOnce
  size: 1Gi
  storageClass: ""

# ===============================
# Scheduling Configuration
# ===============================
nodeSelector: {}

tolerations: []

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - alert-history
        topologyKey: kubernetes.io/hostname
