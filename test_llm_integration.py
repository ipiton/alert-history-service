#!/usr/bin/env python3
"""
–¢–µ—Å—Ç LLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ webhook
"""

import json
import os
import time

import requests

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
os.environ["LLM_ENABLED"] = "true"
os.environ["LLM_API_KEY"] = "sk-eEyKBRlxsrWB81yZT5Mc1w"
os.environ["LLM_PROXY_URL"] = "https://llm-proxy.b2broker.tech"
os.environ["LLM_MODEL"] = "openai/gpt-4o"

BASE_URL = "http://localhost:8000"


def test_llm_integration():
    """–¢–µ—Å—Ç LLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    print("üîç –¢–µ—Å—Ç LLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ webhook")
    print("=" * 50)

    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞
    try:
        response = requests.get(f"{BASE_URL}/healthz")
        if response.status_code == 200:
            print("‚úÖ –°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç")
        else:
            print(f"‚ùå –°–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {response.status_code}")
            return
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å–µ—Ä–≤–∏—Å—É: {e}")
        return

    # 2. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∂–∏–º transparent_with_recommendations
    try:
        response = requests.post(
            f"{BASE_URL}/enrichment/mode",
            json={"mode": "transparent_with_recommendations"},
        )
        if response.status_code == 200:
            print("‚úÖ –†–µ–∂–∏–º transparent_with_recommendations —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–µ–∂–∏–º–∞: {response.status_code}")
            return
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–µ–∂–∏–º–∞: {e}")
        return

    # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º webhook —á–µ—Ä–µ–∑ /webhook/proxy
    webhook_data = {
        "receiver": "test",
        "alerts": [
            {
                "fingerprint": "test-llm-1",
                "status": "firing",
                "labels": {
                    "alertname": "HighCPUUsage",
                    "instance": "server-01",
                    "severity": "warning",
                },
                "annotations": {
                    "summary": "High CPU usage detected",
                    "description": "CPU usage is above 90% for more than 5 minutes",
                },
                "startsAt": "2024-01-01T00:00:00Z",
            }
        ],
    }

    try:
        start_time = time.time()
        response = requests.post(
            f"{BASE_URL}/webhook/proxy", json=webhook_data, timeout=30
        )
        processing_time = time.time() - start_time

        print(f"‚úÖ Webhook –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {processing_time:.2f}s")
        print(f"üìä –°—Ç–∞—Ç—É—Å: {response.status_code}")

        if response.status_code == 200:
            result = response.json()
            print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {json.dumps(result, indent=2)}")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if result.get("classification_results"):
                print("üéâ LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç!")
                for fingerprint, classification in result[
                    "classification_results"
                ].items():
                    print(f"   üîç {fingerprint}: {classification}")
            else:
                print("‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                print("   üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                print("      - LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                print("      - –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π")
                print("      - Timeout –∑–∞–ø—Ä–æ—Å–∞")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ webhook: {response.text}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ webhook: {e}")


if __name__ == "__main__":
    test_llm_integration()
