# Requirements: LLM Alert Classifier & Recommendation System

## Обоснование задачи

### Проблема
В текущей инфраструктуре мониторинга между Alertmanager и системами управления инцидентами (например, Rootly) существует проблема:

1. **Избыточное количество алертов** - многие алерты не требуют немедленного внимания или создают шум
2. **Неточная категоризация** - алерты не классифицируются по важности и типу
3. **Отсутствие интеллектуальных рекомендаций** - нет автоматических советов по настройке алертов
4. **Ручная обработка** - требуется значительное время на анализ и настройку правил алертинга

### Цель
Использовать существующий сервис AlertHistory как основу для интеграции LLM-классификатора, который будет:
- Анализировать паттерны алертов
- Классифицировать алерты по важности и типу
- Предоставлять рекомендации по настройке Alertmanager
- Сокращать количество false-positive алертов

## Пользовательские сценарии

### Сценарий 1: Автоматическая классификация новых алертов
**Как:** DevOps Engineer
**Хочу:** Чтобы новые алерты автоматически классифицировались по важности
**Чтобы:** Сосредоточиться на критичных инцидентах

**Acceptance Criteria:**
- Новые алерты получают метки классификации (critical, warning, info, noise)
- Классификация основана на исторических данных и LLM анализе
- Результат доступен через API и dashboard

### Сценарий 2: Рекомендации по настройке алертов
**Как:** Platform Engineer
**Хочу:** Получать рекомендации по оптимизации правил алертинга
**Чтобы:** Уменьшить количество ложных срабатываний

**Acceptance Criteria:**
- Система анализирует историю flapping алертов
- Предлагает изменения в thresholds, evaluation_interval, repeat_interval
- Рекомендации доступны в структурированном формате

### Сценарий 3: Интеграция с Rootly
**Как:** Incident Manager
**Хочу:** Чтобы в Rootly попадали только действительно важные алерты
**Чтобы:** Не тратить время на анализ шума

**Acceptance Criteria:**
- Алерты с классификацией 'noise' не передаются в Rootly
- Critical алерты получают высокий приоритет
- Контекст классификации передается в метаданных

### Сценарий 4: Переключение режима обработки (Transparent/Enriched)
**Как:** SRE/Platform Engineer
**Хочу:** Переключать режим работы сервиса между прозрачным прокси и обогащённым режимом
**Чтобы:** Включать/выключать LLM-обогащение без релизов, для A/B тестов или maintenance

**Acceptance Criteria:**
- `GET /enrichment/mode` возвращает текущий режим и источник
- `POST /enrichment/mode` меняет режим для всех реплик (через Redis)
- В `/webhook/proxy` режим `transparent` отключает классификацию, `enriched` — включает
- На дашборде есть понятный переключатель и отображение текущего режима

## Функциональные требования

### FR-1: LLM Классификация алертов
- Анализ алертов с использованием OpenAI API (GPT-4)
- Классификация по категориям: critical, warning, info, noise
- Анализ контекста: labels, annotations, исторические данные
- Поддержка function calling для структурированных ответов

### FR-2: Рекомендательная система
- Анализ паттернов flapping алертов
- Рекомендации по настройке thresholds
- Предложения по изменению evaluation_interval
- Советы по корректировке repeat_interval

### FR-3: API для интеграции
- REST API для получения классификации алертов
- Webhook для автоматической обработки новых алертов
- API для получения рекомендаций по настройкам
- Интеграция с существующими эндпоинтами AlertHistory

### FR-5: Режимы обогащения алертов
- ENV `ENRICHMENT_MODE` с default=`enriched`
- Redis-ключ `enrichment:mode` — источник истины, fallback — память узла
- Эндпоинты `GET/POST /enrichment/mode`
- Интеграция режима в пайплайн `/webhook/proxy`

### FR-4: Хранение результатов анализа
- Расширение базы данных для хранения классификации
- Хранение рекомендаций и их статуса
- Версионирование рекомендаций
- Audit log изменений

## Нефункциональные требования

### NFR-1: Производительность
- Время отклика LLM классификации < 5 секунд
- Batch обработка исторических алертов
- Кеширование результатов классификации
- Асинхронная обработка для больших объемов

### NFR-2: Надежность
- Fallback механизм при недоступности OpenAI API
- Retry логика с exponential backoff
- Graceful degradation при ошибках LLM
- Сохранение функциональности AlertHistory при сбоях

### NFR-3: Безопасность
- Использование Kubernetes Secrets для хранения ключей LLM-proxy
- Шифрование чувствительных данных алертов
- Rate limiting через LLM-proxy конфигурацию
- Audit logging всех LLM запросов
- mTLS подключение к LLM-proxy (если требуется)

### NFR-4: Масштабируемость
- Горизонтальное масштабирование классификатора
- Поддержка multiple OpenAI API ключей
- Балансировка нагрузки между LLM вызовами
- Оптимизация для обработки > 1000 алертов/час

### NFR-5: Security & Access
- Для прод: защитить `/enrichment/mode` аутентификацией и RBAC, аудит изменений

## Технические ограничения

### Внешние зависимости
- **LLM Proxy**: Интеграция через внутренний LLM-proxy сервис
- **Kubernetes Secrets**: Хранение API ключей для LLM
- **ConfigMaps**: Хранение настроек LLM классификатора
- **Существующая база AlertHistory**: Совместимость с текущей схемой
- **Alertmanager**: Интеграция через webhooks
- **Rootly**: Совместимость с их API

### Ограничения ресурсов
- **LLM Proxy Rate Limits**: Лимиты определяются конфигурацией proxy
- **Cost Control**: Мониторинг через LLM-proxy метрики
- **Memory**: Эффективное использование для batch обработки
- **Storage**: Дополнительное место для классификаций и рекомендаций
- **Network**: Стабильное подключение к LLM-proxy сервису

### Совместимость
- **Python 3.8+**: Совместимость с текущим стеком
- **FastAPI**: Использование существующего фреймворка
- **SQLite**: Расширение текущей схемы базы данных
- **Kubernetes**: Готовность к деплою через Helm

## Критерии приемки

### Минимально жизнеспособный продукт (MVP)
1. ✅ LLM классификация новых алертов на 4 категории
2. ✅ API эндпоинт для получения классификации: `GET /classify/{alert_id}`
3. ✅ Автоматическая классификация при поступлении webhook
4. ✅ Базовые рекомендации по flapping алертам
5. ✅ Dashboard с отображением классификации
6. ✅ Конфигурация через environment variables

### Расширенная функциональность
1. Batch классификация исторических данных
2. Интеграция с Rootly через API
3. A/B тестирование рекомендаций
4. ML модель для локальной классификации
5. Детальная аналитика эффективности рекомендаций

## Риски и ограничения

### Высокие риски
- **OpenAI API недоступность**: Может заблокировать всю функциональность
- **Высокие costs**: Превышение бюджета на API вызовы
- **Качество классификации**: Неточные результаты LLM

### Средние риски
- **Latency**: Медленная работа при больших объемах
- **Integration complexity**: Сложность интеграции с Rootly
- **Data privacy**: Передача чувствительных данных в OpenAI

### Низкие риски
- **Schema migration**: Необходимость миграции базы данных
- **Backward compatibility**: Совместимость с текущими клиентами

## Метрики успеха

### Ключевые показатели
1. **Снижение шума**: Уменьшение алертов категории 'noise' на 30%
2. **Точность классификации**: Accuracy > 85% на validation set
3. **Время обработки**: < 5 секунд на классификацию алерта
4. **Uptime**: 99.5% доступность сервиса классификации

### Бизнес метрики
1. **Reduction in false positives**: Снижение на 40%
2. **MTTR improvement**: Улучшение на 25%
3. **Engineer satisfaction**: Оценка > 4/5 от команды DevOps
4. **Cost optimization**: ROI > 200% за 6 месяцев

---

**Владелец требований:** DevOps Team Lead
**Дата создания:** 2024-12-28
**Версия:** 1.0
**Статус:** Draft → Review → Approved
