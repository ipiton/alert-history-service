# Design: LLM Alert Classifier & Recommendation System

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ

### –û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (12-Factor + Horizontal Scaling)

```mermaid
graph TB
    subgraph "Kubernetes Cluster"
        subgraph "AlertHistory Namespace"
            subgraph "Application Instances (Stateless)"
                AH1[AlertHistory Pod 1<br/>+ Intelligent Proxy]
                AH2[AlertHistory Pod 2<br/>+ Intelligent Proxy]
                AH3[AlertHistory Pod 3<br/>+ Intelligent Proxy]
            end

            subgraph "Shared State & Config"
                CM[ConfigMap<br/>12-Factor Config]
                SEC[Secret<br/>Credentials]
                SVC[LoadBalancer Service]
            end

            subgraph "Persistent Layer"
                PG[(PostgreSQL<br/>Shared Database)]
                REDIS[(Redis<br/>Caching & Session)]
                PVC[(PVC<br/>Logs & Temp)]
            end
        end

        subgraph "LLM Namespace"
            LP[LLM Proxy Service]
        end

        subgraph "Monitoring"
            AM[Alertmanager]
            GR[Grafana]
        end

        subgraph "External Systems"
            RT[Rootly]
            WH1[Webhook Target 1]
            WH2[Webhook Target 2]
            PD[PagerDuty]
        end
    end

    AM -->|webhook<br/>load balanced| SVC
    SVC -->|distribute| AH1
    SVC -->|distribute| AH2
    SVC -->|distribute| AH3

    AH1 -->|classify| LP
    AH2 -->|classify| LP
    AH3 -->|classify| LP

    LP -->|OpenAI/Azure| LLM[External LLM]

    AH1 -->|store/read| PG
    AH2 -->|store/read| PG
    AH3 -->|store/read| PG

    AH1 -->|cache| REDIS
    AH2 -->|cache| REDIS
    AH3 -->|cache| REDIS

    AH1 -->|read config| CM
    AH2 -->|read config| CM
    AH3 -->|read config| CM

    AH1 -->|publish| RT
    AH2 -->|publish| WH1
    AH3 -->|publish| PD

    GR -->|metrics| SVC
```

## 12-Factor App Principles

### Factor I: Codebase
- **–û–¥–Ω–∞ –∫–æ–¥–æ–≤–∞—è –±–∞–∑–∞** –¥–ª—è –≤—Å–µ—Ö environments (dev, staging, prod)
- **Git repository** —Å branches –¥–ª—è —Ä–∞–∑–Ω—ã—Ö environments
- **Container image** —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑, –¥–µ–ø–ª–æ–∏—Ç—Å—è –≤–µ–∑–¥–µ
- **Helm chart** —É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥

### Factor II: Dependencies
- **Explicit dependencies** –≤ `requirements.txt`
- **No system-wide packages** - –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω—ã
- **Dependency isolation** —á–µ—Ä–µ–∑ Docker containers
- **Lock —Ñ–∞–π–ª—ã** –¥–ª—è reproducible builds
- **PEP8 compliance tools** –≤ dev dependencies (black, flake8, mypy)

### Factor III: Config
- **Environment variables** –¥–ª—è –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
- **No hardcoded config** –≤ –∫–æ–¥–µ
- **ConfigMaps** –¥–ª—è Kubernetes-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
- **Secrets** –¥–ª—è sensitive data (API keys, passwords)

```yaml
# –ü—Ä–∏–º–µ—Ä—ã environment variables
DATABASE_URL: "postgresql://user:pass@postgres:5432/alerthistory"
REDIS_URL: "redis://redis:6379/0"
LLM_PROXY_URL: "http://llm-proxy.llm-namespace:8080"
LLM_PROXY_API_KEY: "${LLM_PROXY_API_KEY}"  # From Secret
LOG_LEVEL: "INFO"
METRICS_ENABLED: "true"
PUBLISHING_ENABLED: "true"
```

### Factor IV: Backing Services
- **PostgreSQL** - –æ—Å–Ω–æ–≤–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (treated as attached resource)
- **Redis** - –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ session storage
- **LLM Proxy** - external service —á–µ—Ä–µ–∑ URL
- **External APIs** (Rootly, PagerDuty) - attached resources
- **All services** accessible via URLs from config

### Factor V: Build, Release, Run
- **Build stage:** Docker image creation with all dependencies
- **Release stage:** Combine build + config (Helm values)
- **Run stage:** Execute in Kubernetes environment
- **Immutable releases** with version tags
- **Rollback capability** —á–µ—Ä–µ–∑ Kubernetes deployments

### Factor VI: Processes
- **Stateless processes** - no local state between requests
- **Session data** stored in Redis (external)
- **Database state** in PostgreSQL (external)
- **File uploads** in PVC or object storage
- **Process crash recovery** handled by Kubernetes

### Factor VII: Port Binding
- **Self-contained service** exports HTTP via port binding
- **No web server dependency** - uses FastAPI/uvicorn internally
- **Service exports** via Kubernetes Service
- **Load balancing** handled by K8s Service

### Factor VIII: Concurrency
- **Horizontal scaling** via Kubernetes replicas
- **Process types:** web (HTTP), worker (background tasks)
- **Stateless design** enables unlimited horizontal scaling
- **No shared memory** between processes

### Factor IX: Disposability
- **Fast startup** - optimized container startup time
- **Graceful shutdown** - proper signal handling (SIGTERM)
- **Robust against sudden death** - idempotent operations
- **Health checks** for liveness/readiness probes

### Factor X: Dev/Prod Parity
- **Same technology stack** across environments
- **Same backing services** (PostgreSQL, Redis)
- **Same deployment method** (Helm charts)
- **Minimal time gap** between dev and prod deployments
- **Continuous deployment** pipeline

### Factor XI: Logs
- **Logs to stdout/stderr** - no log files
- **Structured logging** (JSON format)
- **Log aggregation** handled by Kubernetes/ELK
- **No log rotation** in application
- **Centralized logging** via Fluent Bit/Fluentd

### Factor XII: Admin Processes
- **Database migrations** via init containers
- **One-time scripts** as Kubernetes Jobs
- **Admin tasks** via kubectl exec or dedicated endpoints
- **Same codebase** for admin processes
- **Same environment** for admin and regular processes

## Python Code Quality Standards (PEP8+)

### Code Formatting & Style
- **PEP8 compliance** - —Å—Ç—Ä–æ–≥–æ–µ —Å–æ–±–ª—é–¥–µ–Ω–∏–µ Python style guide
- **Black** - –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞ (line length: 88)
- **isort** - —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ imports
- **flake8** - –ø—Ä–æ–≤–µ—Ä–∫–∞ style violations
- **mypy** - static type checking
- **pylint** - comprehensive code analysis

### Type Annotations
```python
# –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ type hints –¥–ª—è –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π
from typing import Dict, List, Optional, Union, AsyncGenerator
from dataclasses import dataclass
from enum import Enum

class AlertSeverity(Enum):
    """Alert severity levels following PEP8 enum conventions."""
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"
    NOISE = "noise"

@dataclass
class ClassificationResult:
    """Classification result with comprehensive type annotations."""
    severity: AlertSeverity
    confidence: float
    reasoning: str
    recommendations: List[str]
    processing_time: float
    metadata: Optional[Dict[str, Union[str, int, float]]] = None

class AlertClassifier:
    """Alert classifier following PEP8 naming conventions."""

    def __init__(self, llm_client: 'LLMProxyClient', config: Dict[str, str]) -> None:
        """Initialize classifier with proper docstring format."""
        self.llm_client = llm_client
        self.config = config
        self._cache: Dict[str, ClassificationResult] = {}

    async def classify_alert(
        self,
        alert: Dict[str, Union[str, Dict, List]],
        context: Optional[Dict[str, str]] = None
    ) -> ClassificationResult:
        """
        Classify alert using LLM with comprehensive type hints.

        Args:
            alert: Alert data from Alertmanager
            context: Optional context for classification

        Returns:
            Classification result with severity and reasoning

        Raises:
            ValueError: If alert data is invalid
            ConnectionError: If LLM service is unavailable
        """
        if not alert.get('labels', {}).get('alertname'):
            raise ValueError("Alert must contain alertname in labels")

        # Implementation following PEP8 conventions
        alert_key = self._generate_cache_key(alert)

        if alert_key in self._cache:
            return self._cache[alert_key]

        try:
            result = await self._perform_classification(alert, context)
            self._cache[alert_key] = result
            return result
        except Exception as exc:
            logger.error(f"Classification failed for alert {alert_key}: {exc}")
            raise

    def _generate_cache_key(self, alert: Dict[str, Union[str, Dict, List]]) -> str:
        """Generate cache key following PEP8 private method conventions."""
        # Private method implementation
        pass

    async def _perform_classification(
        self,
        alert: Dict[str, Union[str, Dict, List]],
        context: Optional[Dict[str, str]]
    ) -> ClassificationResult:
        """Perform actual classification - private async method."""
        # Implementation
        pass
```

### Documentation Standards
```python
"""
Module: alert_classifier

This module provides intelligent alert classification using LLM.

Classes:
    AlertClassifier: Main classification engine
    ClassificationResult: Result dataclass
    AlertSeverity: Severity enumeration

Functions:
    create_classifier: Factory function for classifier instances

Example:
    >>> classifier = create_classifier(llm_client, config)
    >>> result = await classifier.classify_alert(alert_data)
    >>> print(result.severity.value)
    'critical'
"""

import asyncio
import logging
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Union, AsyncGenerator

# Standard library imports first
# Third-party imports second
# Local imports last (following isort conventions)
```

### Error Handling Patterns
```python
class AlertClassificationError(Exception):
    """Base exception for alert classification errors."""
    pass

class LLMServiceError(AlertClassificationError):
    """LLM service unavailable or returned error."""
    pass

class InvalidAlertDataError(AlertClassificationError):
    """Alert data is invalid or incomplete."""
    pass

# Proper exception handling with context
async def classify_alert_with_error_handling(
    self,
    alert: Dict[str, Union[str, Dict, List]]
) -> ClassificationResult:
    """Classify alert with comprehensive error handling."""
    try:
        return await self.classify_alert(alert)
    except ValueError as exc:
        raise InvalidAlertDataError(f"Invalid alert data: {exc}") from exc
    except ConnectionError as exc:
        raise LLMServiceError(f"LLM service unavailable: {exc}") from exc
    except Exception as exc:
        logger.exception("Unexpected error during classification")
        raise AlertClassificationError(f"Classification failed: {exc}") from exc
```

### Async/Await Best Practices
```python
import asyncio
from contextlib import asynccontextmanager
from typing import AsyncGenerator

class AlertProcessor:
    """Alert processor with proper async patterns."""

    async def __aenter__(self) -> 'AlertProcessor':
        """Async context manager entry."""
        await self._initialize_connections()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Async context manager exit with proper cleanup."""
        await self._cleanup_connections()

    @asynccontextmanager
    async def processing_session(self) -> AsyncGenerator['ProcessingSession', None]:
        """Async context manager for processing sessions."""
        session = ProcessingSession()
        try:
            await session.start()
            yield session
        finally:
            await session.close()

    async def process_alerts_batch(
        self,
        alerts: List[Dict[str, Union[str, Dict, List]]]
    ) -> List[ClassificationResult]:
        """Process alerts in batch with proper concurrency control."""
        semaphore = asyncio.Semaphore(10)  # Limit concurrent operations

        async def process_single_alert(alert: Dict) -> ClassificationResult:
            async with semaphore:
                return await self.classify_alert(alert)

        tasks = [process_single_alert(alert) for alert in alerts]
        return await asyncio.gather(*tasks, return_exceptions=True)
```

### Logging Standards
```python
import logging
import json
from datetime import datetime
from typing import Any, Dict

# Structured logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',  # JSON format in stdout
    handlers=[logging.StreamHandler()]
)

logger = logging.getLogger(__name__)

class StructuredLogger:
    """Structured JSON logger for 12-factor compliance."""

    @staticmethod
    def log_classification(
        alert_fingerprint: str,
        severity: AlertSeverity,
        confidence: float,
        processing_time: float,
        **kwargs: Any
    ) -> None:
        """Log classification with structured format."""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event": "alert_classified",
            "alert_fingerprint": alert_fingerprint,
            "severity": severity.value,
            "confidence": confidence,
            "processing_time": processing_time,
            "service": "alert-history-llm",
            **kwargs
        }
        logger.info(json.dumps(log_entry))

    @staticmethod
    def log_error(
        operation: str,
        error: Exception,
        alert_fingerprint: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """Log error with structured format."""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event": "error",
            "operation": operation,
            "error_type": type(error).__name__,
            "error_message": str(error),
            "service": "alert-history-llm",
            **kwargs
        }
        if alert_fingerprint:
            log_entry["alert_fingerprint"] = alert_fingerprint

        logger.error(json.dumps(log_entry))
```

### Testing Standards
```python
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from typing import AsyncGenerator

class TestAlertClassifier:
    """Test class following pytest conventions."""

    @pytest.fixture
    async def classifier(self) -> AsyncGenerator[AlertClassifier, None]:
        """Fixture for classifier instance."""
        mock_llm_client = AsyncMock()
        config = {"model": "gpt-4", "timeout": 30}

        classifier = AlertClassifier(mock_llm_client, config)
        yield classifier

        # Cleanup if needed
        await classifier.cleanup()

    @pytest.mark.asyncio
    async def test_classify_alert_success(self, classifier: AlertClassifier) -> None:
        """Test successful alert classification."""
        # Arrange
        alert_data = {
            "labels": {"alertname": "CPUHigh"},
            "status": "firing"
        }

        # Act
        result = await classifier.classify_alert(alert_data)

        # Assert
        assert isinstance(result, ClassificationResult)
        assert result.severity in AlertSeverity
        assert 0.0 <= result.confidence <= 1.0
        assert isinstance(result.reasoning, str)
        assert len(result.reasoning) > 0

    @pytest.mark.asyncio
    async def test_classify_alert_invalid_data(self, classifier: AlertClassifier) -> None:
        """Test classification with invalid alert data."""
        # Arrange
        invalid_alert = {"status": "firing"}  # Missing alertname

        # Act & Assert
        with pytest.raises(InvalidAlertDataError):
            await classifier.classify_alert(invalid_alert)

    @pytest.mark.parametrize("alert_name,expected_severity", [
        ("CPUHigh", AlertSeverity.WARNING),
        ("ServiceDown", AlertSeverity.CRITICAL),
        ("DiskSpaceInfo", AlertSeverity.INFO),
    ])
    async def test_classify_different_severities(
        self,
        classifier: AlertClassifier,
        alert_name: str,
        expected_severity: AlertSeverity
    ) -> None:
        """Parametrized test for different alert severities."""
        # Implementation
        pass
```

### Requirements –¥–ª—è Code Quality
```python
# requirements-dev.txt
black==23.12.1              # Code formatting
isort==5.13.2               # Import sorting
flake8==6.1.0               # Style checking
mypy==1.8.0                 # Static type checking
pylint==3.0.3               # Comprehensive analysis
pytest==7.4.3              # Testing framework
pytest-asyncio==0.21.1     # Async testing
pytest-cov==4.1.0          # Coverage reporting
pre-commit==3.6.0          # Git hooks
bandit==1.7.5               # Security linting

# pyproject.toml configuration
[tool.black]
line-length = 88
target-version = ['py38', 'py39', 'py310', 'py311']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["alert_history"]

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[tool.pylint]
max-line-length = 88
disable = [
    "C0114",  # missing-module-docstring
    "R0903",  # too-few-public-methods
]

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "--cov=alert_history --cov-report=html --cov-report=term"

[tool.coverage.run]
source = ["alert_history"]
omit = ["tests/*", "*/test_*"]
```

## Horizontal Scaling Architecture

### Stateless Application Design

```python
# –ü—Ä–∏–º–µ—Ä stateless application design
class StatelessAlertProcessor:
    def __init__(self):
        # –í—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ environment variables
        self.db_url = os.environ['DATABASE_URL']
        self.redis_url = os.environ['REDIS_URL']
        self.llm_proxy_url = os.environ['LLM_PROXY_URL']

        # Shared connections (connection pooling)
        self.db_pool = create_async_pool(self.db_url)
        self.redis_pool = aioredis.from_url(self.redis_url)

        # No local state
        self.session_data = None  # Always fetch from Redis

    async def process_alert(self, alert_data: Dict) -> Dict:
        """Stateless alert processing - no instance state"""
        # Get session data from Redis (if needed)
        session_key = f"session:{alert_data.get('fingerprint')}"
        session_data = await self.redis_pool.get(session_key)

        # Process alert (stateless)
        result = await self._classify_and_publish(alert_data)

        # Store result in database (shared state)
        await self._store_to_database(result)

        # Update cache (shared state)
        await self.redis_pool.setex(
            f"classification:{alert_data.get('fingerprint')}",
            3600,
            json.dumps(result)
        )

        return result
```

### Database Architecture for Multiple Instances

```sql
-- Database schema optimized for concurrent access
CREATE TABLE alert_history (
    id BIGSERIAL PRIMARY KEY,
    fingerprint TEXT NOT NULL,
    alertname TEXT NOT NULL,
    status TEXT NOT NULL,
    labels JSONB NOT NULL,
    raw_json JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    processed_by TEXT,  -- Instance ID for debugging

    -- Optimistic locking for concurrent updates
    version INTEGER DEFAULT 1,

    -- Indexes for performance
    CONSTRAINT unique_fingerprint_status UNIQUE (fingerprint, status, created_at)
);

-- Separate table for classifications (to avoid conflicts)
CREATE TABLE alert_classifications (
    id BIGSERIAL PRIMARY KEY,
    alert_fingerprint TEXT NOT NULL,
    severity TEXT NOT NULL,
    confidence REAL NOT NULL,
    reasoning TEXT,
    model_version TEXT,
    classified_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    classified_by TEXT,  -- Instance ID

    -- Prevent duplicate classifications
    CONSTRAINT unique_classification UNIQUE (alert_fingerprint, model_version)
);

-- Publishing log for tracking
CREATE TABLE publishing_log (
    id BIGSERIAL PRIMARY KEY,
    alert_fingerprint TEXT NOT NULL,
    target_name TEXT NOT NULL,
    target_url TEXT NOT NULL,
    payload JSONB NOT NULL,
    status TEXT NOT NULL,  -- success, failed, retrying
    response_code INTEGER,
    response_body TEXT,
    published_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    published_by TEXT,  -- Instance ID
    retry_count INTEGER DEFAULT 0
);

-- Distributed locking table
CREATE TABLE distributed_locks (
    lock_name TEXT PRIMARY KEY,
    locked_by TEXT NOT NULL,  -- Instance ID
    locked_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
    lock_data JSONB
);
```

### Redis Caching Strategy

```python
class DistributedCache:
    """Redis-based distributed caching for multiple instances"""

    def __init__(self, redis_pool):
        self.redis = redis_pool
        self.instance_id = os.environ.get('HOSTNAME', 'unknown')

    async def get_classification(self, fingerprint: str) -> Optional[Dict]:
        """Get cached classification result"""
        cache_key = f"classification:{fingerprint}"
        cached = await self.redis.get(cache_key)
        return json.loads(cached) if cached else None

    async def set_classification(self, fingerprint: str, result: Dict, ttl: int = 3600):
        """Cache classification result with TTL"""
        cache_key = f"classification:{fingerprint}"
        await self.redis.setex(cache_key, ttl, json.dumps(result))

    async def acquire_lock(self, lock_name: str, timeout: int = 30) -> bool:
        """Distributed lock for critical sections"""
        lock_key = f"lock:{lock_name}"
        acquired = await self.redis.set(
            lock_key,
            self.instance_id,
            ex=timeout,
            nx=True
        )
        return bool(acquired)

    async def release_lock(self, lock_name: str):
        """Release distributed lock"""
        lock_key = f"lock:{lock_name}"
        # Lua script for atomic release
        script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
        """
        await self.redis.eval(script, 1, lock_key, self.instance_id)
```

### Horizontal Pod Autoscaler Configuration

```yaml
# HPA for automatic scaling based on metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: alert-history-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: alert-history
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "30"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
```

### Load Balancing and Session Affinity

```yaml
# Service configuration for proper load balancing
apiVersion: v1
kind: Service
metadata:
  name: alert-history
spec:
  selector:
    app: alert-history
  ports:
  - port: 8080
    targetPort: 8080
  sessionAffinity: None  # No session affinity for stateless design
  type: ClusterIP

# Ingress for external access with load balancing
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: alert-history-ingress
  annotations:
    nginx.ingress.kubernetes.io/upstream-hash-by: "$request_uri"  # Consistent hashing
    nginx.ingress.kubernetes.io/load-balance: "round_robin"
spec:
  rules:
  - host: alert-history.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: alert-history
            port:
              number: 8080
```

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

#### 1. Intelligent Alert Proxy (Stateless)
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ü—Ä–∏–µ–º, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
- `AlertReceiver` - –ø—Ä–∏–µ–º webhook –æ—Ç Alertmanager
- `AlertClassifier` - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ LLM
- `AlertEnricher` - –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
- `AlertPublisher` - –ø—É–±–ª–∏–∫–∞—Ü–∏—è –≤ downstream —Å–∏—Å—Ç–µ–º—ã
- `FilterEngine` - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

#### 2. LLM Classifier Module
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
- `AlertClassifier` - –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- `RecommendationEngine` - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º
- `LLMProxyClient` - –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM-proxy
- `ClassificationCache` - –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- `AlertAnalyzer` - –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–ª–µ—Ä—Ç–æ–≤

#### 3. Alert Publisher
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ü—É–±–ª–∏–∫–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
- `WebhookPublisher` - –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ webhook endpoints
- `AlertFormatter` - —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
- `TargetManager` - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
- `DeliveryTracker` - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–æ—Å—Ç–∞–≤–∫–∏

#### 4. LLM Proxy Integration
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º LLM-proxy —Å–µ—Ä–≤–∏—Å–æ–º

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤—ã–∑–æ–≤–æ–≤ LLM-proxy
- Retry –º–µ—Ö–∞–Ω–∏–∑–º —Å exponential backoff
- Circuit breaker pattern –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
- –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–∑–æ–≤–æ–≤

#### 5. Configuration Management
**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ Kubernetes —Ä–µ—Å—É—Ä—Å—ã

**ConfigMap —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alert-history-llm-config
data:
  llm-proxy-url: "http://llm-proxy.llm-namespace:8080"
  classification-enabled: "true"
  recommendations-enabled: "true"
  publishing-enabled: "true"
  batch-size: "10"
  cache-ttl: "3600"
  models: |
    classification: "gpt-4"
    recommendations: "gpt-4"
  prompts: |
    classification: |
      Analyze this Prometheus alert and classify it into one of these categories:
      - critical: Requires immediate attention, service is down or severely degraded
      - warning: Performance issue or potential problem, needs investigation
      - info: Informational alert, good to know but not urgent
      - noise: False positive or irrelevant alert

      Alert data: {alert_data}
      Historical context: {historical_context}
    recommendations: |
      Based on this alert pattern analysis, provide recommendations for Alertmanager configuration:
      Alert: {alert_name}
      Flapping count: {flap_count}
      Average duration: {avg_duration}

      Suggest improvements for: evaluation_interval, for duration, repeat_interval
    # Publishing configuration (targets discovered dynamically from secrets)
  publishing-discovery:
    enabled: true
    secret-labels:
      - "publishing-target=true"  # Label selector for discovering target secrets
    secret-namespaces:
      - "default"  # Namespaces to search for target secrets
      - "alert-history"
    config-refresh-interval: "300s"  # How often to reload target configs

  # Default publishing configuration (fallback if no secrets found)
  publishing-defaults:
    metrics-only-mode: true  # If no targets configured, only collect metrics
    fallback-targets: |
      # Example targets that can be enabled via secrets
      # These serve as templates and documentation
      - name: "rootly-template"
        type: "webhook"
        description: "Rootly integration template"
        format: "rootly"
        enabled: false  # Disabled by default, enable via secret
        filter:
          severity: ["critical", "warning"]
          exclude_noise: true
        required_secrets:
          - "rootly-api-key"
          - "rootly-webhook-url"

      - name: "pagerduty-template"
        type: "webhook"
        description: "PagerDuty integration template"
        format: "pagerduty"
        enabled: false
        filter:
          severity: ["critical"]
          namespaces: ["production", "staging"]
        required_secrets:
          - "pagerduty-routing-key"

      - name: "slack-template"
        type: "webhook"
        description: "Slack webhook template"
        format: "slack"
        enabled: false
        filter:
          severity: ["critical"]
          exclude_noise: true
        required_secrets:
          - "slack-webhook-url"

      - name: "custom-webhook-template"
        type: "webhook"
        description: "Generic webhook template"
        format: "alertmanager"
        enabled: false
        filter:
          severity: ["critical", "warning", "info"]
        required_secrets:
          - "webhook-url"
```

**Secret —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
```yaml
# Main LLM secrets
apiVersion: v1
kind: Secret
metadata:
  name: alert-history-llm-secrets
type: Opaque
data:
  llm-proxy-api-key: <base64-encoded-key>
  llm-proxy-auth-token: <base64-encoded-token>

---
# Dynamic publishing targets secrets (optional)
apiVersion: v1
kind: Secret
metadata:
  name: alert-history-publishing-secrets
type: Opaque
data:
  # Rootly integration (optional)
  rootly-api-key: <base64-encoded-key>
  rootly-webhook-url: <base64-encoded-url>

  # PagerDuty integration (optional)
  pagerduty-routing-key: <base64-encoded-key>
  pagerduty-token: <base64-encoded-token>

  # Slack webhooks (optional, can be multiple)
  slack-webhook-critical: <base64-encoded-url>
  slack-webhook-general: <base64-encoded-url>

  # Custom webhook targets (optional, dynamic)
  webhook-target-1-url: <base64-encoded-url>
  webhook-target-1-auth: <base64-encoded-header>
  webhook-target-2-url: <base64-encoded-url>
  webhook-target-2-auth: <base64-encoded-header>

  # Additional targets can be added dynamically
  # Format: {target-name}-{property}: value

---
# Alternative: Multiple secrets for different teams/environments
apiVersion: v1
kind: Secret
metadata:
  name: team-a-publishing-secrets
  labels:
    publishing-target: "true"
    team: "team-a"
type: Opaque
data:
  target-name: dGVhbS1hLXdlYmhvb2s=  # team-a-webhook
  webhook-url: <base64-encoded-url>
  auth-header: <base64-encoded-auth>
  enabled: dHJ1ZQ==  # true

---
apiVersion: v1
kind: Secret
metadata:
  name: team-b-publishing-secrets
  labels:
    publishing-target: "true"
    team: "team-b"
type: Opaque
data:
  target-name: dGVhbS1iLXNsYWNr  # team-b-slack
  webhook-url: <base64-encoded-url>
  format: c2xhY2s=  # slack
  enabled: dHJ1ZQ==  # true
```

## –î–µ—Ç–∞–ª—å–Ω—ã–π –¥–∏–∑–∞–π–Ω –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

### 1. AlertClassifier –∫–ª–∞—Å—Å

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum

class AlertSeverity(Enum):
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"
    NOISE = "noise"

@dataclass
class ClassificationResult:
    severity: AlertSeverity
    confidence: float
    reasoning: str
    recommendations: List[str]
    processing_time: float

class AlertClassifier:
    def __init__(self, llm_client: LLMProxyClient, config: Dict):
        self.llm_client = llm_client
        self.config = config
        self.cache = ClassificationCache()

    async def classify_alert(self, alert: Dict, context: Dict = None) -> ClassificationResult:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∞–ª–µ—Ä—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
        pass

    async def batch_classify(self, alerts: List[Dict]) -> List[ClassificationResult]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–æ–≤"""
        pass

    def _prepare_prompt(self, alert: Dict, context: Dict) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM"""
        pass
```

### 2. LLMProxyClient –∫–ª–∞—Å—Å

```python
import aiohttp
import asyncio
from typing import Dict, Any
import logging

class LLMProxyClient:
    def __init__(self, base_url: str, api_key: str, auth_token: str = None):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.auth_token = auth_token
        self.session = None
        self.circuit_breaker = CircuitBreaker()

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers=self._get_auth_headers(),
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def call_llm(self, model: str, messages: List[Dict],
                      functions: List[Dict] = None) -> Dict[str, Any]:
        """–í—ã–∑–æ–≤ LLM —á–µ—Ä–µ–∑ proxy —Å retry –ª–æ–≥–∏–∫–æ–π"""

        @self.circuit_breaker
        async def _make_request():
            async with self.session.post(
                f"{self.base_url}/v1/chat/completions",
                json={
                    "model": model,
                    "messages": messages,
                    "functions": functions,
                    "function_call": "auto" if functions else None
                }
            ) as response:
                response.raise_for_status()
                return await response.json()

        return await self._retry_with_backoff(_make_request)

    def _get_auth_headers(self) -> Dict[str, str]:
        headers = {"Authorization": f"Bearer {self.api_key}"}
        if self.auth_token:
            headers["X-Auth-Token"] = self.auth_token
        return headers

    async def _retry_with_backoff(self, func, max_retries: int = 3):
        """Retry —Å exponential backoff"""
        for attempt in range(max_retries):
            try:
                return await func()
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                wait_time = 2 ** attempt
                await asyncio.sleep(wait_time)
```

### 3. RecommendationEngine –∫–ª–∞—Å—Å

```python
@dataclass
class AlertRecommendation:
    alert_name: str
    namespace: str
    current_config: Dict
    recommendations: Dict
    impact_estimate: str
    confidence: float

class RecommendationEngine:
    def __init__(self, llm_client: LLMProxyClient, db_client):
        self.llm_client = llm_client
        self.db = db_client

    async def analyze_flapping_alerts(self, days: int = 7) -> List[AlertRecommendation]:
        """–ê–Ω–∞–ª–∏–∑ flapping –∞–ª–µ—Ä—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        flapping_alerts = await self._get_flapping_alerts(days)
        recommendations = []

        for alert in flapping_alerts:
            context = await self._build_alert_context(alert)
            recommendation = await self._generate_recommendation(alert, context)
            recommendations.append(recommendation)

        return recommendations

    async def _generate_recommendation(self, alert: Dict, context: Dict) -> AlertRecommendation:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ LLM"""
        prompt = self._build_recommendation_prompt(alert, context)

        functions = [{
            "name": "provide_alertmanager_recommendations",
            "description": "Provide recommendations for Alertmanager configuration",
            "parameters": {
                "type": "object",
                "properties": {
                    "evaluation_interval": {"type": "string"},
                    "for_duration": {"type": "string"},
                    "repeat_interval": {"type": "string"},
                    "threshold_adjustment": {"type": "string"},
                    "reasoning": {"type": "string"},
                    "impact_estimate": {"type": "string"}
                },
                "required": ["reasoning", "impact_estimate"]
            }
        }]

        response = await self.llm_client.call_llm(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            functions=functions
        )

        return self._parse_recommendation_response(alert, response)

### 4. AlertPublisher –∏ AlertFormatter –∫–ª–∞—Å—Å—ã

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
from enum import Enum
import aiohttp
import asyncio

class PublishingFormat(Enum):
    ALERTMANAGER = "alertmanager"
    ROOTLY = "rootly"
    PAGERDUTY = "pagerduty"
    SLACK = "slack"
    WEBHOOK = "webhook"

@dataclass
class PublishingTarget:
    name: str
    type: str
    url: str
    enabled: bool
    filter_config: Dict
    headers: Dict
    format: PublishingFormat
    retry_config: Optional[Dict] = None

@dataclass
class EnrichedAlert:
    """–ê–ª–µ—Ä—Ç –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    original_alert: Dict
    classification: ClassificationResult
    enriched_labels: Dict
    enriched_annotations: Dict
    processing_metadata: Dict

class AlertFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º"""

    def format_for_alertmanager(self, alert: EnrichedAlert) -> Dict:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Alertmanager —Ñ–æ—Ä–º–∞—Ç —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏"""
        formatted = alert.original_alert.copy()

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –≤ labels
        if 'labels' not in formatted:
            formatted['labels'] = {}

        formatted['labels'].update({
            'llm_severity': alert.classification.severity.value,
            'llm_confidence': str(alert.classification.confidence),
            'processed_by': 'alert-history-llm'
        })

        # –î–æ–±–∞–≤–ª—è–µ–º reasoning –≤ annotations
        if 'annotations' not in formatted:
            formatted['annotations'] = {}

        formatted['annotations'].update({
            'llm_reasoning': alert.classification.reasoning,
            'llm_recommendations': ', '.join(alert.classification.recommendations),
            'classification_timestamp': alert.processing_metadata.get('classified_at')
        })

        return formatted

    def format_for_rootly(self, alert: EnrichedAlert) -> Dict:
        """–§–æ—Ä–º–∞—Ç –¥–ª—è Rootly API"""
        return {
            "data": {
                "type": "alerts",
                "attributes": {
                    "title": alert.original_alert.get('labels', {}).get('alertname', 'Unknown Alert'),
                    "description": alert.original_alert.get('annotations', {}).get('description', ''),
                    "severity": self._map_severity_to_rootly(alert.classification.severity),
                    "source": "alertmanager",
                    "labels": alert.original_alert.get('labels', {}),
                    "annotations": alert.original_alert.get('annotations', {}),
                    "classification": {
                        "severity": alert.classification.severity.value,
                        "confidence": alert.classification.confidence,
                        "reasoning": alert.classification.reasoning
                    },
                    "fingerprint": alert.original_alert.get('fingerprint'),
                    "starts_at": alert.original_alert.get('startsAt'),
                    "ends_at": alert.original_alert.get('endsAt')
                }
            }
        }

    def format_for_pagerduty(self, alert: EnrichedAlert) -> Dict:
        """–§–æ—Ä–º–∞—Ç –¥–ª—è PagerDuty Events API v2"""
        severity_map = {
            AlertSeverity.CRITICAL: "critical",
            AlertSeverity.WARNING: "warning",
            AlertSeverity.INFO: "info",
            AlertSeverity.NOISE: "info"
        }

        return {
            "routing_key": "${PAGERDUTY_ROUTING_KEY}",  # –ó–∞–º–µ–Ω—è–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            "event_action": "trigger" if alert.original_alert.get('status') == 'firing' else "resolve",
            "dedup_key": alert.original_alert.get('fingerprint'),
            "payload": {
                "summary": alert.original_alert.get('labels', {}).get('alertname', 'Unknown Alert'),
                "source": alert.original_alert.get('generatorURL', 'alertmanager'),
                "severity": severity_map.get(alert.classification.severity, "warning"),
                "component": alert.original_alert.get('labels', {}).get('job', 'unknown'),
                "group": alert.original_alert.get('labels', {}).get('namespace', 'default'),
                "class": alert.classification.severity.value,
                "custom_details": {
                    "labels": alert.original_alert.get('labels', {}),
                    "annotations": alert.original_alert.get('annotations', {}),
                    "classification": {
                        "confidence": alert.classification.confidence,
                        "reasoning": alert.classification.reasoning
                    }
                }
            }
        }

    def format_for_slack(self, alert: EnrichedAlert) -> Dict:
        """–§–æ—Ä–º–∞—Ç –¥–ª—è Slack webhook"""
        color_map = {
            AlertSeverity.CRITICAL: "danger",
            AlertSeverity.WARNING: "warning",
            AlertSeverity.INFO: "good",
            AlertSeverity.NOISE: "#D3D3D3"
        }

        return {
            "attachments": [{
                "color": color_map.get(alert.classification.severity, "warning"),
                "title": f"üö® {alert.original_alert.get('labels', {}).get('alertname', 'Alert')}",
                "text": alert.original_alert.get('annotations', {}).get('description', ''),
                "fields": [
                    {
                        "title": "Severity",
                        "value": alert.classification.severity.value.upper(),
                        "short": True
                    },
                    {
                        "title": "Confidence",
                        "value": f"{alert.classification.confidence:.2f}",
                        "short": True
                    },
                    {
                        "title": "Namespace",
                        "value": alert.original_alert.get('labels', {}).get('namespace', 'unknown'),
                        "short": True
                    },
                    {
                        "title": "LLM Reasoning",
                        "value": alert.classification.reasoning[:200] + "..." if len(alert.classification.reasoning) > 200 else alert.classification.reasoning,
                        "short": False
                    }
                ],
                "footer": "Alert History LLM Classifier",
                "ts": int(time.time())
            }]
        }

    def _map_severity_to_rootly(self, severity: AlertSeverity) -> str:
        mapping = {
            AlertSeverity.CRITICAL: "critical",
            AlertSeverity.WARNING: "major",
            AlertSeverity.INFO: "minor",
            AlertSeverity.NOISE: "low"
        }
        return mapping.get(severity, "major")

class FilterEngine:
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–æ–≤ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º"""

    def should_publish(self, alert: EnrichedAlert, target: PublishingTarget) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –∞–ª–µ—Ä—Ç –≤ —Ü–µ–ª—å"""
        filter_config = target.filter_config

        # –§–∏–ª—å—Ç—Ä –ø–æ severity
        if 'severity' in filter_config:
            allowed_severities = filter_config['severity']
            if alert.classification.severity.value not in allowed_severities:
                return False

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ noise –∞–ª–µ—Ä—Ç–æ–≤
        if filter_config.get('exclude_noise', False):
            if alert.classification.severity == AlertSeverity.NOISE:
                return False

        # –§–∏–ª—å—Ç—Ä –ø–æ namespace
        if 'namespaces' in filter_config:
            alert_namespace = alert.original_alert.get('labels', {}).get('namespace')
            if alert_namespace not in filter_config['namespaces']:
                return False

        # –§–∏–ª—å—Ç—Ä –ø–æ alertname patterns
        if 'alertname_patterns' in filter_config:
            import re
            alertname = alert.original_alert.get('labels', {}).get('alertname', '')
            patterns = filter_config['alertname_patterns']
            if not any(re.match(pattern, alertname) for pattern in patterns):
                return False

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å confidence
        if 'min_confidence' in filter_config:
            min_confidence = filter_config['min_confidence']
            if alert.classification.confidence < min_confidence:
                return False

        return True

class AlertPublisher:
    """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã"""

    def __init__(self, targets: List[PublishingTarget], formatter: AlertFormatter):
        self.targets = targets
        self.formatter = formatter
        self.filter_engine = FilterEngine()
        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def publish_alert(self, alert: EnrichedAlert) -> Dict[str, bool]:
        """–ü—É–±–ª–∏–∫—É–µ—Ç –∞–ª–µ—Ä—Ç –≤–æ –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ü–µ–ª–∏"""
        results = {}

        # –ü—É–±–ª–∏–∫—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤–æ –≤—Å–µ —Ü–µ–ª–∏
        tasks = []
        for target in self.targets:
            if target.enabled and self.filter_engine.should_publish(alert, target):
                task = self._publish_to_target(alert, target)
                tasks.append((target.name, task))

        if tasks:
            task_results = await asyncio.gather(
                *[task for _, task in tasks],
                return_exceptions=True
            )

            for (target_name, _), result in zip(tasks, task_results):
                results[target_name] = not isinstance(result, Exception)
                if isinstance(result, Exception):
                    logging.error(f"Failed to publish to {target_name}: {result}")

        return results

    async def _publish_to_target(self, alert: EnrichedAlert, target: PublishingTarget) -> bool:
        """–ü—É–±–ª–∏–∫—É–µ—Ç –∞–ª–µ—Ä—Ç –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ü–µ–ª—å"""
        try:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∞–ª–µ—Ä—Ç –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            if target.format == PublishingFormat.ALERTMANAGER:
                payload = self.formatter.format_for_alertmanager(alert)
            elif target.format == PublishingFormat.ROOTLY:
                payload = self.formatter.format_for_rootly(alert)
            elif target.format == PublishingFormat.PAGERDUTY:
                payload = self.formatter.format_for_pagerduty(alert)
            elif target.format == PublishingFormat.SLACK:
                payload = self.formatter.format_for_slack(alert)
            else:
                payload = self.formatter.format_for_alertmanager(alert)

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å
            headers = target.headers.copy()
            # –ó–∞–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤ headers
            for key, value in headers.items():
                if isinstance(value, str) and '${' in value:
                    headers[key] = self._substitute_env_vars(value)

            async with self.session.post(
                target.url,
                json=payload,
                headers=headers
            ) as response:
                response.raise_for_status()

                # –ú–µ—Ç—Ä–∏–∫–∏
                ALERT_PUBLISHER_SUCCESS.labels(target=target.name, format=target.format.value).inc()
                return True

        except Exception as e:
            ALERT_PUBLISHER_ERRORS.labels(target=target.name, format=target.format.value).inc()
            logging.error(f"Failed to publish to {target.name}: {e}")
            raise

    def _substitute_env_vars(self, text: str) -> str:
        """–ó–∞–º–µ–Ω—è–µ—Ç ${VAR_NAME} –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        import re
        import os

        def replace_var(match):
            var_name = match.group(1)
            return os.environ.get(var_name, match.group(0))

        return re.sub(r'\$\{([^}]+)\}', replace_var, text)

### 5. DynamicTargetManager –∫–ª–∞—Å—Å

```python
import asyncio
from kubernetes import client, config, watch
from typing import Dict, List, Optional, Set
import base64
import yaml
import logging

class DynamicTargetManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ publishing targets —á–µ—Ä–µ–∑ Kubernetes secrets"""

    def __init__(self, config_manager):
        self.config = config_manager
        self.k8s_client = None
        self.current_targets: Dict[str, PublishingTarget] = {}
        self.watched_secrets: Set[str] = set()
        self.refresh_task = None

        # Load Kubernetes config
        try:
            config.load_incluster_config()  # Running in cluster
        except:
            config.load_kube_config()  # Development

        self.k8s_client = client.CoreV1Api()

    async def start_monitoring(self):
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ secrets –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è targets"""
        self.refresh_task = asyncio.create_task(self._refresh_loop())
        await self._discover_and_load_targets()

    async def stop_monitoring(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        if self.refresh_task:
            self.refresh_task.cancel()

    async def _refresh_loop(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ targets"""
        interval = self.config.get('publishing-discovery.config-refresh-interval', '300s')
        interval_seconds = self._parse_duration(interval)

        while True:
            try:
                await asyncio.sleep(interval_seconds)
                await self._discover_and_load_targets()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logging.error(f"Error in target refresh loop: {e}")

    async def _discover_and_load_targets(self):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ publishing targets –∏–∑ secrets"""
        try:
            discovered_targets = await self._discover_targets_from_secrets()

            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —Ç–µ–∫—É—â–∏–º–∏ targets
            if discovered_targets != self.current_targets:
                old_count = len(self.current_targets)
                self.current_targets = discovered_targets
                new_count = len(self.current_targets)

                logging.info(f"Publishing targets updated: {old_count} -> {new_count}")

                # –ú–µ—Ç—Ä–∏–∫–∏
                PUBLISHING_TARGETS_DISCOVERED.set(new_count)

                # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
                for target_name, target in self.current_targets.items():
                    logging.info(f"Active publishing target: {target_name} -> {target.url}")

        except Exception as e:
            logging.error(f"Failed to discover publishing targets: {e}")

    async def _discover_targets_from_secrets(self) -> Dict[str, PublishingTarget]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ targets –∏–∑ Kubernetes secrets"""
        targets = {}

        # Get discovery configuration
        discovery_config = self.config.get('publishing-discovery', {})
        if not discovery_config.get('enabled', False):
            logging.info("Publishing target discovery disabled")
            return targets

        label_selectors = discovery_config.get('secret-labels', [])
        namespaces = discovery_config.get('secret-namespaces', ['default'])

        for namespace in namespaces:
            for label_selector in label_selectors:
                try:
                    secrets = self.k8s_client.list_namespaced_secret(
                        namespace=namespace,
                        label_selector=label_selector
                    )

                    for secret in secrets.items:
                        target = await self._create_target_from_secret(secret, namespace)
                        if target:
                            targets[target.name] = target

                except Exception as e:
                    logging.error(f"Error listing secrets in {namespace}: {e}")

        return targets

    async def _create_target_from_secret(self, secret, namespace: str) -> Optional[PublishingTarget]:
        """–°–æ–∑–¥–∞–Ω–∏–µ PublishingTarget –∏–∑ Kubernetes secret"""
        try:
            secret_data = secret.data or {}

            # Decode base64 values
            decoded_data = {}
            for key, value in secret_data.items():
                try:
                    decoded_data[key] = base64.b64decode(value).decode('utf-8')
                except Exception as e:
                    logging.warning(f"Failed to decode secret key {key}: {e}")
                    continue

            # Extract target configuration
            target_name = decoded_data.get('target-name', secret.metadata.name)
            webhook_url = decoded_data.get('webhook-url') or decoded_data.get('url')

            if not webhook_url:
                logging.warning(f"No webhook URL found in secret {secret.metadata.name}")
                return None

            # Check if target is enabled
            enabled = decoded_data.get('enabled', 'true').lower() == 'true'
            if not enabled:
                logging.info(f"Target {target_name} is disabled")
                return None

            # Build headers
            headers = {'Content-Type': 'application/json'}

            # Authentication
            if 'auth-header' in decoded_data:
                auth_parts = decoded_data['auth-header'].split(' ', 1)
                if len(auth_parts) == 2:
                    headers['Authorization'] = decoded_data['auth-header']
            elif 'api-key' in decoded_data:
                headers['Authorization'] = f"Bearer {decoded_data['api-key']}"
            elif 'token' in decoded_data:
                headers['Authorization'] = f"Token {decoded_data['token']}"

            # Additional headers
            for key, value in decoded_data.items():
                if key.startswith('header-'):
                    header_name = key[7:].replace('-', '-').title()
                    headers[header_name] = value

            # Extract format
            format_str = decoded_data.get('format', 'alertmanager')
            try:
                target_format = PublishingFormat(format_str)
            except ValueError:
                target_format = PublishingFormat.ALERTMANAGER

            # Extract filter configuration
            filter_config = {}

            # Severity filter
            if 'filter-severity' in decoded_data:
                filter_config['severity'] = decoded_data['filter-severity'].split(',')
            elif 'severity' in decoded_data:
                filter_config['severity'] = decoded_data['severity'].split(',')

            # Namespace filter
            if 'filter-namespaces' in decoded_data:
                filter_config['namespaces'] = decoded_data['filter-namespaces'].split(',')
            elif 'namespaces' in decoded_data:
                filter_config['namespaces'] = decoded_data['namespaces'].split(',')

            # Exclude noise
            exclude_noise = decoded_data.get('exclude-noise', 'true').lower() == 'true'
            filter_config['exclude_noise'] = exclude_noise

            # Min confidence
            if 'min-confidence' in decoded_data:
                try:
                    filter_config['min_confidence'] = float(decoded_data['min-confidence'])
                except ValueError:
                    pass

            # Create target
            target = PublishingTarget(
                name=target_name,
                type="webhook",
                url=webhook_url,
                enabled=True,
                filter_config=filter_config,
                headers=headers,
                format=target_format
            )

            logging.info(f"Created publishing target from secret: {target_name}")
            return target

        except Exception as e:
            logging.error(f"Failed to create target from secret {secret.metadata.name}: {e}")
            return None

    def get_active_targets(self) -> List[PublishingTarget]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ publishing targets"""
        return list(self.current_targets.values())

    def get_target_by_name(self, name: str) -> Optional[PublishingTarget]:
        """–ü–æ–ª—É—á–∏—Ç—å target –ø–æ –∏–º–µ–Ω–∏"""
        return self.current_targets.get(name)

    def get_targets_count(self) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö targets"""
        return len(self.current_targets)

    def is_metrics_only_mode(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —Å–µ—Ä–≤–∏—Å –≤ —Ä–µ–∂–∏–º–µ —Ç–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫"""
        return len(self.current_targets) == 0

    def _parse_duration(self, duration: str) -> int:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ –≤ —Å–µ–∫—É–Ω–¥—ã"""
        if duration.endswith('s'):
            return int(duration[:-1])
        elif duration.endswith('m'):
            return int(duration[:-1]) * 60
        elif duration.endswith('h'):
            return int(duration[:-1]) * 3600
        else:
            return int(duration)  # Assume seconds

class MetricsOnlyMode:
    """–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Ç–æ–ª—å–∫–æ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (–±–µ–∑ publishing)"""

    @staticmethod
    def create_enriched_alert(alert: Dict, classification: ClassificationResult) -> EnrichedAlert:
        """–°–æ–∑–¥–∞—Ç—å –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –∞–ª–µ—Ä—Ç –¥–ª—è –º–µ—Ç—Ä–∏–∫"""
        return EnrichedAlert(
            original_alert=alert,
            classification=classification,
            enriched_labels={
                'llm_severity': classification.severity.value,
                'llm_confidence': str(classification.confidence),
                'processed_by': 'alert-history-llm'
            },
            enriched_annotations={
                'llm_reasoning': classification.reasoning,
                'classification_timestamp': datetime.utcnow().isoformat()
            },
            processing_metadata={
                'mode': 'metrics-only',
                'targets_available': 0
            }
        )

    @staticmethod
    def record_metrics(alert: EnrichedAlert):
        """–ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–ª–µ—Ä—Ç–∞"""
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        ALERTS_ENRICHED.labels(severity=alert.classification.severity.value).inc()
        PROXY_PROCESSING_TIME.observe(0.1)  # Minimal processing time

        # –ú–µ—Ç—Ä–∏–∫–∞ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã
        METRICS_ONLY_MODE.set(1)

    @staticmethod
    def get_dashboard_data() -> Dict:
        """–î–∞–Ω–Ω—ã–µ –¥–ª—è dashboard –≤ —Ä–µ–∂–∏–º–µ —Ç–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫"""
        return {
            "mode": "metrics-only",
            "publishing_enabled": False,
            "active_targets": 0,
            "message": "Service running in metrics-only mode. No publishing targets configured."
        }
```
```

### 4. Database Schema Extensions

**–ù–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã:**

```sql
-- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–ª–µ—Ä—Ç–æ–≤
CREATE TABLE alert_classifications (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    alert_id INTEGER REFERENCES alert_history(id),
    severity TEXT NOT NULL, -- critical, warning, info, noise
    confidence REAL NOT NULL,
    reasoning TEXT,
    model_version TEXT,
    classified_at TEXT NOT NULL,
    FOREIGN KEY (alert_id) REFERENCES alert_history(id)
);

-- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º
CREATE TABLE alert_recommendations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    alert_name TEXT NOT NULL,
    namespace TEXT,
    recommendation_type TEXT, -- flapping, threshold, duration
    current_config TEXT, -- JSON
    recommended_config TEXT, -- JSON
    reasoning TEXT,
    confidence REAL,
    status TEXT DEFAULT 'pending', -- pending, applied, rejected
    created_at TEXT NOT NULL,
    applied_at TEXT
);

-- –ö–µ—à –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π
CREATE TABLE classification_cache (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    alert_hash TEXT UNIQUE NOT NULL, -- hash –∞–ª–µ—Ä—Ç–∞ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
    classification_result TEXT NOT NULL, -- JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    expires_at TEXT NOT NULL,
    created_at TEXT NOT NULL
);

-- –ú–µ—Ç—Ä–∏–∫–∏ LLM –≤—ã–∑–æ–≤–æ–≤
CREATE TABLE llm_metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    operation_type TEXT NOT NULL, -- classification, recommendation
    model_name TEXT NOT NULL,
    tokens_used INTEGER,
    processing_time REAL,
    success BOOLEAN,
    error_message TEXT,
    created_at TEXT NOT NULL
);
```

### 5. API Endpoints

**–ù–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã:**

```python
# --- INTELLIGENT PROXY ENDPOINTS ---

@app.post("/webhook/proxy")
async def intelligent_webhook_proxy(request: Request):
    """
    Intelligent proxy –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤ –æ—Ç Alertmanager:
    1. –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∞–ª–µ—Ä—Ç—ã –æ—Ç Alertmanager
    2. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —á–µ—Ä–µ–∑ LLM
    3. –û–±–æ–≥–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    4. –ü—É–±–ª–∏–∫—É–µ—Ç –≤ downstream —Å–∏—Å—Ç–µ–º—ã
    5. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
    """
    start = time.time()
    try:
        payload = await request.json()
        alerts = payload.get("alerts", [])

        processed_alerts = []
        publishing_results = {}

        async with AlertPublisher(publishing_targets, AlertFormatter()) as publisher:
            for alert in alerts:
                # 1. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∞–ª–µ—Ä—Ç
                classification = await classifier.classify_alert(alert)

                # 2. –û–±–æ–≥–∞—â–∞–µ–º –∞–ª–µ—Ä—Ç
                enriched_alert = EnrichedAlert(
                    original_alert=alert,
                    classification=classification,
                    enriched_labels={},
                    enriched_annotations={},
                    processing_metadata={
                        'classified_at': datetime.utcnow().isoformat(),
                        'processing_time': time.time() - start
                    }
                )

                # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (–∫–∞–∫ –æ–±—ã—á–Ω–æ)
                await save_to_history(alert, classification)

                # 4. –ü—É–±–ª–∏–∫—É–µ–º –≤ downstream —Å–∏—Å—Ç–µ–º—ã
                publish_results = await publisher.publish_alert(enriched_alert)
                publishing_results[alert.get('fingerprint', 'unknown')] = publish_results

                processed_alerts.append({
                    'fingerprint': alert.get('fingerprint'),
                    'classification': classification.severity.value,
                    'confidence': classification.confidence,
                    'published_to': list(publish_results.keys())
                })

        return {
            "result": "ok",
            "processed": len(processed_alerts),
            "alerts": processed_alerts,
            "publishing_results": publishing_results
        }

    except Exception as e:
        logging.error(f"Intelligent proxy error: {e}")
        # Fallback - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        await webhook(request)  # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π handler
        raise HTTPException(status_code=500, detail=f"Proxy processing failed: {e}")

# --- DYNAMIC PUBLISHING MANAGEMENT ---

@app.get("/publishing/targets")
async def get_publishing_targets() -> Dict:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∞–ª–µ—Ä—Ç–æ–≤"""
    targets = target_manager.get_active_targets()

    return {
        "targets": [
            {
                "name": target.name,
                "type": target.type,
                "url": target.url[:50] + "..." if len(target.url) > 50 else target.url,  # Truncate for security
                "format": target.format.value,
                "enabled": target.enabled,
                "filter_config": target.filter_config
            }
            for target in targets
        ],
        "total_count": len(targets),
        "metrics_only_mode": target_manager.is_metrics_only_mode(),
        "discovery_enabled": target_manager.config.get('publishing-discovery.enabled', False)
    }

@app.post("/publishing/targets/refresh")
async def refresh_publishing_targets() -> Dict:
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ publishing targets –∏–∑ secrets"""
    old_count = target_manager.get_targets_count()
    await target_manager._discover_and_load_targets()
    new_count = target_manager.get_targets_count()

    return {
        "result": "ok",
        "targets_before": old_count,
        "targets_after": new_count,
        "refreshed_at": datetime.utcnow().isoformat()
    }

@app.get("/publishing/mode")
async def get_publishing_mode() -> Dict:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã publishing"""
    targets_count = target_manager.get_targets_count()

    return {
        "mode": "metrics-only" if targets_count == 0 else "publishing",
        "targets_count": targets_count,
        "metrics_only": targets_count == 0,
        "available_targets": [t.name for t in target_manager.get_active_targets()]
    }

@app.get("/publishing/stats")
async def get_publishing_stats(
    since: Optional[str] = None,
    until: Optional[str] = None,
    target_name: Optional[str] = None
) -> Dict:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∞–ª–µ—Ä—Ç–æ–≤ –ø–æ —Ü–µ–ª—è–º"""
    # Query publishing log from database
    query = "SELECT target_name, status, COUNT(*) as count FROM publishing_log WHERE 1=1"
    params = []

    if since:
        query += " AND published_at >= ?"
        params.append(since)
    if until:
        query += " AND published_at <= ?"
        params.append(until)
    if target_name:
        query += " AND target_name = ?"
        params.append(target_name)

    query += " GROUP BY target_name, status ORDER BY target_name, status"

    with get_db() as conn:
        c = conn.cursor()
        c.execute(query, params)
        rows = c.fetchall()

    # Aggregate stats
    stats_by_target = {}
    total_stats = {"success": 0, "failed": 0, "total": 0}

    for target, status, count in rows:
        if target not in stats_by_target:
            stats_by_target[target] = {"success": 0, "failed": 0, "total": 0}

        stats_by_target[target][status] = count
        stats_by_target[target]["total"] += count
        total_stats[status] += count
        total_stats["total"] += count

    return {
        "period": {"since": since, "until": until},
        "total_stats": total_stats,
        "by_target": stats_by_target,
        "active_targets": target_manager.get_targets_count(),
        "metrics_only_mode": target_manager.is_metrics_only_mode()
    }

@app.post("/publishing/test/{target_name}")
async def test_publishing_target(target_name: str, test_payload: Optional[Dict] = None) -> Dict:
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—É–±–ª–∏–∫–∞—Ü–∏—é –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ü–µ–ª—å"""
    target = target_manager.get_target_by_name(target_name)
    if not target:
        raise HTTPException(status_code=404, detail=f"Publishing target {target_name} not found")

    # Create test alert if not provided
    if not test_payload:
        test_payload = {
            "alertname": "TestAlert",
            "status": "firing",
            "labels": {
                "alertname": "TestAlert",
                "namespace": "test",
                "severity": "info"
            },
            "annotations": {
                "description": "Test alert for publishing target validation"
            },
            "fingerprint": "test-fingerprint",
            "startsAt": datetime.utcnow().isoformat(),
            "generatorURL": "http://test"
        }

    # Create mock classification
    test_classification = ClassificationResult(
        severity=AlertSeverity.INFO,
        confidence=0.95,
        reasoning="Test classification for publishing validation",
        recommendations=["Test recommendation"],
        processing_time=0.1
    )

    # Create enriched alert
    enriched_alert = EnrichedAlert(
        original_alert=test_payload,
        classification=test_classification,
        enriched_labels={},
        enriched_annotations={},
        processing_metadata={"test_mode": True}
    )

    # Test publishing
    try:
        async with AlertPublisher([target], AlertFormatter()) as publisher:
            results = await publisher.publish_alert(enriched_alert)

        return {
            "target_name": target_name,
            "test_result": "success" if results.get(target_name, False) else "failed",
            "target_url": target.url,
            "target_format": target.format.value,
            "tested_at": datetime.utcnow().isoformat(),
            "details": results
        }

    except Exception as e:
        return {
            "target_name": target_name,
            "test_result": "failed",
            "error": str(e),
            "tested_at": datetime.utcnow().isoformat()
        }

@app.get("/publishing/secrets/template")
async def get_secret_template(format: str = "rootly") -> Dict:
    """–ü–æ–ª—É—á–∞–µ—Ç template –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è secret –¥–ª—è publishing target"""
    templates = {
        "rootly": {
            "apiVersion": "v1",
            "kind": "Secret",
            "metadata": {
                "name": "rootly-publishing-target",
                "labels": {
                    "publishing-target": "true",
                    "team": "your-team"
                }
            },
            "type": "Opaque",
            "stringData": {
                "target-name": "rootly-production",
                "webhook-url": "https://api.rootly.com/v1/alerts",
                "api-key": "your-rootly-api-key",
                "format": "rootly",
                "enabled": "true",
                "filter-severity": "critical,warning",
                "exclude-noise": "true"
            }
        },
        "pagerduty": {
            "apiVersion": "v1",
            "kind": "Secret",
            "metadata": {
                "name": "pagerduty-publishing-target",
                "labels": {
                    "publishing-target": "true",
                    "team": "your-team"
                }
            },
            "type": "Opaque",
            "stringData": {
                "target-name": "pagerduty-critical",
                "webhook-url": "https://events.pagerduty.com/v2/enqueue",
                "routing-key": "your-pagerduty-routing-key",
                "format": "pagerduty",
                "enabled": "true",
                "filter-severity": "critical",
                "filter-namespaces": "production,staging"
            }
        },
        "slack": {
            "apiVersion": "v1",
            "kind": "Secret",
            "metadata": {
                "name": "slack-publishing-target",
                "labels": {
                    "publishing-target": "true",
                    "team": "your-team"
                }
            },
            "type": "Opaque",
            "stringData": {
                "target-name": "slack-alerts",
                "webhook-url": "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK",
                "format": "slack",
                "enabled": "true",
                "filter-severity": "critical,warning",
                "exclude-noise": "true"
            }
        },
        "webhook": {
            "apiVersion": "v1",
            "kind": "Secret",
            "metadata": {
                "name": "custom-webhook-target",
                "labels": {
                    "publishing-target": "true",
                    "team": "your-team"
                }
            },
            "type": "Opaque",
            "stringData": {
                "target-name": "custom-webhook",
                "webhook-url": "https://your-webhook-endpoint.com/alerts",
                "auth-header": "Bearer your-auth-token",
                "format": "alertmanager",
                "enabled": "true",
                "filter-severity": "critical,warning,info",
                "min-confidence": "0.8"
            }
        }
    }

    template = templates.get(format)
    if not template:
        raise HTTPException(status_code=400, detail=f"Unknown template format: {format}")

    return {
        "format": format,
        "template": template,
        "available_formats": list(templates.keys()),
        "usage": f"kubectl apply -f {format}-secret.yaml"
    }

# --- CLASSIFICATION ENDPOINTS ---

@app.post("/classify")
async def classify_alert(alert_data: Dict) -> ClassificationResult:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∞–ª–µ—Ä—Ç –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    pass

@app.get("/classification/{alert_id}")
async def get_classification(alert_id: int) -> ClassificationResult:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∞–ª–µ—Ä—Ç–∞"""
    pass

@app.post("/classify/batch")
async def batch_classify(alert_ids: List[int]) -> List[ClassificationResult]:
    """–ü–∞–∫–µ—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∞–ª–µ—Ä—Ç–æ–≤"""
    pass

# --- RECOMMENDATIONS ENDPOINTS ---

@app.get("/recommendations")
async def get_recommendations(
    alert_name: Optional[str] = None,
    namespace: Optional[str] = None,
    status: Optional[str] = None
) -> List[AlertRecommendation]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º"""
    pass

@app.post("/recommendations/generate")
async def generate_recommendations() -> Dict[str, int]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
    pass

@app.patch("/recommendations/{rec_id}/status")
async def update_recommendation_status(rec_id: int, status: str) -> Dict:
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (applied/rejected)"""
    pass

# --- ENRICHED HISTORY ---

@app.get("/history/enriched")
async def get_enriched_history(
    alertname: Optional[str] = Query(None),
    severity: Optional[str] = Query(None),  # critical, warning, info, noise
    min_confidence: Optional[float] = Query(None),
    namespace: Optional[str] = Query(None),
    since: Optional[str] = Query(None),
    until: Optional[str] = Query(None),
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0)
) -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∞–ª–µ—Ä—Ç–æ–≤ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ–º"""
    pass
```

## Helm Chart —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è

### ConfigMap template

```yaml
# templates/configmap-llm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "alert-history.fullname" . }}-llm-config
  labels:
    {{- include "alert-history.labels" . | nindent 4 }}
data:
  llm-proxy-url: {{ .Values.llm.proxyUrl | quote }}
  classification-enabled: {{ .Values.llm.classification.enabled | quote }}
  recommendations-enabled: {{ .Values.llm.recommendations.enabled | quote }}
  batch-size: {{ .Values.llm.batchSize | quote }}
  cache-ttl: {{ .Values.llm.cacheTtl | quote }}
  models: |
    classification: {{ .Values.llm.models.classification | quote }}
    recommendations: {{ .Values.llm.models.recommendations | quote }}
  prompts: |
    {{- .Values.llm.prompts | toYaml | nindent 4 }}
  retry-config: |
    max-retries: {{ .Values.llm.retry.maxRetries }}
    backoff-multiplier: {{ .Values.llm.retry.backoffMultiplier }}
    max-wait: {{ .Values.llm.retry.maxWait }}
```

### Secret template

```yaml
# templates/secret-llm.yaml
{{- if .Values.llm.enabled }}
apiVersion: v1
kind: Secret
metadata:
  name: {{ include "alert-history.fullname" . }}-llm-secrets
  labels:
    {{- include "alert-history.labels" . | nindent 4 }}
type: Opaque
data:
  {{- if .Values.llm.auth.apiKey }}
  llm-proxy-api-key: {{ .Values.llm.auth.apiKey | b64enc }}
  {{- end }}
  {{- if .Values.llm.auth.authToken }}
  llm-proxy-auth-token: {{ .Values.llm.auth.authToken | b64enc }}
  {{- end }}
{{- end }}
```

### Values.yaml —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è

```yaml
# –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫ values.yaml –¥–ª—è 12-Factor + Horizontal Scaling
replicaCount: 3  # Minimum replicas for HA

image:
  repository: alert-history
  pullPolicy: IfNotPresent
  tag: "latest"

# 12-Factor Configuration
config:
  # Factor III: Config via environment variables
  logLevel: "INFO"
  metricsEnabled: true
  publishingEnabled: true

  # Database configuration (Factor IV: Backing Services)
  database:
    enabled: true
    type: "postgresql"  # Changed from SQLite
    host: "postgresql"
    port: 5432
    name: "alerthistory"
    username: "alerthistory"
    # Password from secret

  # Redis configuration for caching and sessions
  redis:
    enabled: true
    host: "redis"
    port: 6379
    database: 0
    # Password from secret if needed

# Horizontal Pod Autoscaler
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  # Custom metrics
  customMetrics:
    - type: Pods
      pods:
        metric:
          name: requests_per_second
        target:
          type: AverageValue
          averageValue: "30"

# Resource limits for proper HPA
resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 200m
    memory: 256Mi

# LLM Configuration
llm:
  enabled: true
  proxyUrl: "http://llm-proxy.llm-namespace:8080"

  classification:
    enabled: true
    autoClassify: true

  recommendations:
    enabled: true
    analyzeFlapping: true

  models:
    classification: "gpt-4"
    recommendations: "gpt-4"

  batchSize: 10
  cacheTtl: 3600

  auth:
    apiKey: ""  # Set via --set or external secret
    authToken: ""  # Optional additional auth

  retry:
    maxRetries: 3
    backoffMultiplier: 2
    maxWait: 30

# PostgreSQL dependency
postgresql:
  enabled: true
  auth:
    postgresPassword: "postgres"
    username: "alerthistory"
    password: "alerthistory"
    database: "alerthistory"
  primary:
    persistence:
      enabled: true
      size: 8Gi
    resources:
      requests:
        memory: 256Mi
        cpu: 250m

# Redis dependency
redis:
  enabled: true
  auth:
    enabled: false  # Enable if needed
  master:
    persistence:
      enabled: true
      size: 2Gi
    resources:
      requests:
        memory: 128Mi
        cpu: 100m

# Health checks for Factor IX: Disposability
livenessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5

readinessProbe:
  httpGet:
    path: /healthz
    port: http
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3

# Graceful shutdown
terminationGracePeriodSeconds: 30

# Service configuration
service:
  type: ClusterIP
  port: 8080
  sessionAffinity: None  # Stateless design

# Ingress configuration
ingress:
  enabled: false
  annotations:
    nginx.ingress.kubernetes.io/load-balance: "round_robin"
  hosts:
    - host: alert-history.local
      paths:
        - path: /
          pathType: Prefix

  prompts:
    classification: |
      Analyze this Prometheus alert and classify it into one of these categories:
      - critical: Requires immediate attention, service is down or severely degraded
      - warning: Performance issue or potential problem, needs investigation
      - info: Informational alert, good to know but not urgent
      - noise: False positive or irrelevant alert

      Alert data: {alert_data}
      Historical context: {historical_context}

      Respond with your classification and reasoning.

    recommendations: |
      Based on this alert pattern analysis, provide recommendations for Alertmanager configuration:
      Alert: {alert_name}
      Flapping count: {flap_count}
      Average duration: {avg_duration}

      Suggest improvements for: evaluation_interval, for duration, repeat_interval, thresholds
```

### Deployment –∏–∑–º–µ–Ω–µ–Ω–∏—è

```yaml
# –í templates/deployment.yaml –¥–æ–±–∞–≤–ª—è–µ–º:
spec:
  template:
    spec:
      containers:
      - name: {{ .Chart.Name }}
        env:
        {{- if .Values.llm.enabled }}
        - name: LLM_ENABLED
          value: "true"
        - name: LLM_PROXY_URL
          valueFrom:
            configMapKeyRef:
              name: {{ include "alert-history.fullname" . }}-llm-config
              key: llm-proxy-url
        - name: LLM_API_KEY
          valueFrom:
            secretKeyRef:
              name: {{ include "alert-history.fullname" . }}-llm-secrets
              key: llm-proxy-api-key
        {{- if .Values.llm.auth.authToken }}
        - name: LLM_AUTH_TOKEN
          valueFrom:
            secretKeyRef:
              name: {{ include "alert-history.fullname" . }}-llm-secrets
              key: llm-proxy-auth-token
        {{- end }}
        {{- end }}
        volumeMounts:
        {{- if .Values.llm.enabled }}
        - name: llm-config
          mountPath: /app/config/llm
          readOnly: true
        {{- end }}
      volumes:
      {{- if .Values.llm.enabled }}
      - name: llm-config
        configMap:
          name: {{ include "alert-history.fullname" . }}-llm-config
      {{- end }}
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏

### Prometheus –º–µ—Ç—Ä–∏–∫–∏

```python
# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è LLM —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
LLM_REQUESTS = Counter('alert_history_llm_requests_total',
                      'Total LLM requests', ['operation', 'model', 'status'])
LLM_LATENCY = Histogram('alert_history_llm_latency_seconds',
                       'LLM request latency', ['operation', 'model'])
LLM_TOKENS = Counter('alert_history_llm_tokens_total',
                    'Total tokens used', ['operation', 'model'])
CLASSIFICATION_ACCURACY = Gauge('alert_history_classification_accuracy',
                              'Classification accuracy rate')
CACHE_HITS = Counter('alert_history_cache_hits_total',
                    'Cache hits', ['type'])

# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è publishing —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
ALERT_PUBLISHER_SUCCESS = Counter('alert_history_publisher_success_total',
                                 'Successful alert publications', ['target', 'format'])
ALERT_PUBLISHER_ERRORS = Counter('alert_history_publisher_errors_total',
                                'Failed alert publications', ['target', 'format'])
ALERT_PUBLISHER_LATENCY = Histogram('alert_history_publisher_latency_seconds',
                                   'Alert publishing latency', ['target', 'format'])
ALERTS_FILTERED = Counter('alert_history_alerts_filtered_total',
                         'Alerts filtered out', ['target', 'reason'])
ALERTS_ENRICHED = Counter('alert_history_alerts_enriched_total',
                         'Alerts enriched with classification', ['severity'])
PROXY_PROCESSING_TIME = Histogram('alert_history_proxy_processing_seconds',
                                 'Total proxy processing time per alert')

# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö publishing targets
PUBLISHING_TARGETS_DISCOVERED = Gauge('alert_history_publishing_targets_discovered',
                                     'Number of discovered publishing targets')
METRICS_ONLY_MODE = Gauge('alert_history_metrics_only_mode',
                         'Whether service is running in metrics-only mode (1) or with publishing (0)')
TARGET_DISCOVERY_ERRORS = Counter('alert_history_target_discovery_errors_total',
                                 'Errors during target discovery from secrets')
SECRET_REFRESH_DURATION = Histogram('alert_history_secret_refresh_duration_seconds',
                                   'Time spent refreshing publishing targets from secrets')
PUBLISHING_TARGET_STATUS = Gauge('alert_history_publishing_target_status',
                                'Status of individual publishing targets (1=active, 0=inactive)',
                                ['target_name', 'target_format'])
```

## –°—Ü–µ–Ω–∞—Ä–∏–∏ –æ—à–∏–±–æ–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è

### 1. LLM-proxy –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
- **Fallback:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- **Behavior:** –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–∫–∏, –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –±–∞–∑–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
- **Recovery:** Auto-retry —Å exponential backoff

### 2. –ù–µ–≤–µ—Ä–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- **Detection:** –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏, user feedback
- **Mitigation:** A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤, fine-tuning
- **Recovery:** –û—Ç–∫–∞—Ç –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤

## –†–µ–∂–∏–º—ã –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∞–ª–µ—Ä—Ç–æ–≤ (Transparent vs Enriched)

### –û–±–∑–æ—Ä
- –†–µ–∂–∏–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞, –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π –º–µ–∂–¥—É —Ä–µ–ø–ª–∏–∫–∞–º–∏.
- –ò—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã ‚Äî Redis –∫–ª—é—á `enrichment:mode`. Fallback ‚Äî in-memory `app_state.enrichment_mode` –∏ `ENRICHMENT_MODE` (default=`enriched`).

### API
- `GET /enrichment/mode` ‚Üí `{ mode: "transparent"|"enriched", source: "redis"|"memory"|"default" }`
- `POST /enrichment/mode` body `{ mode: "transparent"|"enriched" }` ‚Üí —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ Redis –∏–ª–∏ –ø–∞–º—è—Ç—å

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ –ø–∞–π–ø–ª–∞–π–Ω `/webhook/proxy`
- –ï—Å–ª–∏ `transparent`: –≤–æ –≤—Ä–µ–º—è —à–∞–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–ª–µ—Ä—Ç–æ–≤ –æ—Ç–∫–ª—é—á–∞–µ–º `enable_auto_classification` —É `WebhookProcessor` (–±–µ–∑ LLM), –æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ (—Ñ–∏–ª—å—Ç—Ä—ã/–ø—É–±–ª–∏–∫–∞—Ü–∏–∏) —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è.
- –ï—Å–ª–∏ `enriched`: —Ç–µ–∫—É—â–∏–π behavior ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞.

### –î–∞—à–±–æ—Ä–¥
- –°–µ–∫—Ü–∏—è Publishing –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–Ω–æ–ø–∫–∏ –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ API.

### –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
- –í –ø—Ä–æ–¥-–æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö ‚Äî –∑–∞—â–∏—Ç–∏—Ç—å `/enrichment/mode` –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π, RBAC –∏ –∞—É–¥–∏—Ç–æ–º. –í –¥–µ–º–æ ‚Äî –¥–æ—Å—Ç—É–ø–Ω–æ –±–µ–∑ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏.

### 3. –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–æ–≤
- **Prevention:** Rate limiting, batch optimization
- **Handling:** Queue —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
- **Monitoring:** –ê–ª–µ—Ä—Ç—ã –ø–æ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—é –ª–∏–º–∏—Ç–æ–≤

---

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä:** DevOps Team
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 2024-12-28
**–í–µ—Ä—Å–∏—è:** 1.0
**–°—Ç–∞—Ç—É—Å:** Draft ‚Üí Technical Review ‚Üí Implementation Ready
