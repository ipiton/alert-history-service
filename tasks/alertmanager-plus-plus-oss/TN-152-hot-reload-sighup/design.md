# TN-152: Hot Reload Mechanism (SIGHUP) - Technical Design

**Date**: 2025-11-22
**Task ID**: TN-152
**Quality Target**: 150% (Grade A+ EXCEPTIONAL)
**Status**: ğŸ“‹ Design Phase
**Estimated Effort**: 6-8 hours

---

## ğŸ—ï¸ Architecture Overview

### High-Level Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Operating System                                 â”‚
â”‚                                                                          â”‚
â”‚  User/Process sends SIGHUP:                                             â”‚
â”‚  $ kill -HUP $(pidof alert-history)                                     â”‚
â”‚  $ kubectl exec alert-history -- kill -HUP 1                            â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ SIGHUP Signal
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Signal Handler Goroutine                              â”‚
â”‚              cmd/server/main.go::setupSignalHandlers()                   â”‚
â”‚                                                                          â”‚
â”‚  sighup := make(chan os.Signal, 1)                                      â”‚
â”‚  signal.Notify(sighup, syscall.SIGHUP)                                  â”‚
â”‚                                                                          â”‚
â”‚  go func() {                                                             â”‚
â”‚      for sig := range sighup {                                          â”‚
â”‚          slog.Info("SIGHUP received, triggering reload")                â”‚
â”‚          reloadChan <- struct{}{}                                       â”‚
â”‚      }                                                                   â”‚
â”‚  }()                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ Reload Trigger
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Reload Coordinator                                   â”‚
â”‚              internal/config/reload_coordinator.go                       â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Phase 1: LOAD & PARSE (Target: < 50ms)                            â”‚â”‚
â”‚  â”‚  â€¢ Read config.yaml from filesystem                                â”‚â”‚
â”‚  â”‚  â€¢ Detect format (YAML/JSON)                                       â”‚â”‚
â”‚  â”‚  â€¢ Parse to Config struct                                          â”‚â”‚
â”‚  â”‚  â€¢ Apply environment variable overrides                            â”‚â”‚
â”‚  â”‚  â€¢ Calculate SHA256 hash                                           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                             â”‚ âœ… Parsed                                  â”‚
â”‚                             â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Phase 2: VALIDATION (Target: < 100ms)                             â”‚â”‚
â”‚  â”‚  â€¢ Structural validation (validator tags)                          â”‚â”‚
â”‚  â”‚  â€¢ Business rules validation                                       â”‚â”‚
â”‚  â”‚  â€¢ Cross-field validation                                          â”‚â”‚
â”‚  â”‚  â€¢ Reference validation (receivers exist)                          â”‚â”‚
â”‚  â”‚  â€¢ Security validation (no hardcoded secrets)                      â”‚â”‚
â”‚  â”‚  âŒ If validation fails â†’ ABORT, keep old config                  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                             â”‚ âœ… Valid                                   â”‚
â”‚                             â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Phase 3: DIFF CALCULATION (Target: < 20ms)                        â”‚â”‚
â”‚  â”‚  â€¢ Deep compare old vs new config                                  â”‚â”‚
â”‚  â”‚  â€¢ Identify changed sections                                       â”‚â”‚
â”‚  â”‚  â€¢ Determine affected components                                   â”‚â”‚
â”‚  â”‚  â€¢ Sanitize secrets in diff                                        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                             â”‚ âœ… Diff Ready                              â”‚
â”‚                             â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Phase 4: ATOMIC APPLY (Target: < 50ms)                            â”‚â”‚
â”‚  â”‚  â€¢ Acquire distributed lock (Redis, 30s timeout)                   â”‚â”‚
â”‚  â”‚  â€¢ Backup old config to storage                                    â”‚â”‚
â”‚  â”‚  â€¢ Swap in-memory config (atomic pointer)                          â”‚â”‚
â”‚  â”‚  â€¢ Increment version counter                                       â”‚â”‚
â”‚  â”‚  â€¢ Write audit log entry                                           â”‚â”‚
â”‚  â”‚  â€¢ Release lock                                                     â”‚â”‚
â”‚  â”‚  âš ï¸  If error â†’ ROLLBACK                                           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                             â”‚ âœ… Applied                                 â”‚
â”‚                             â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Phase 5: COMPONENT RELOAD (Target: < 300ms)                       â”‚â”‚
â”‚  â”‚  â€¢ Notify all registered Reloadable components                     â”‚â”‚
â”‚  â”‚  â€¢ Parallel reload with 30s timeout per component                  â”‚â”‚
â”‚  â”‚  â€¢ Collect reload results                                          â”‚â”‚
â”‚  â”‚  â€¢ Check for critical failures                                     â”‚â”‚
â”‚  â”‚  âš ï¸  If critical component fails â†’ ROLLBACK                        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                             â”‚ âœ… Reloaded                                â”‚
â”‚                             â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Phase 6: HEALTH CHECK (Target: < 50ms)                            â”‚â”‚
â”‚  â”‚  â€¢ Verify all critical components healthy                          â”‚â”‚
â”‚  â”‚  â€¢ Check database connectivity                                     â”‚â”‚
â”‚  â”‚  â€¢ Check Redis connectivity                                        â”‚â”‚
â”‚  â”‚  â€¢ Verify routing engine operational                               â”‚â”‚
â”‚  â”‚  âš ï¸  If health check fails â†’ ROLLBACK                              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                             â”‚                                            â”‚
â”‚                             â–¼                                            â”‚
â”‚                    âœ… RELOAD SUCCESSFUL                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â–¼                 â–¼                  â–¼        â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
                    â”‚   Routing    â”‚  â”‚  Receivers   â”‚  â”‚  Inhibition  â”‚  â”‚ ...  â”‚
                    â”‚   Engine     â”‚  â”‚   Manager    â”‚  â”‚   Manager    â”‚  â”‚      â”‚
                    â”‚  (Critical)  â”‚  â”‚  (Critical)  â”‚  â”‚(Non-Critical)â”‚  â”‚      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                  â”‚                  â”‚
                         â–¼                  â–¼                  â–¼
                    Reload(ctx, cfg)  Reload(ctx, cfg)  Reload(ctx, cfg)
```

---

## ğŸ“¦ Component Design

### 1. Signal Handler Setup (main.go)

**File**: `go-app/cmd/server/main.go`

**Integration Point**: ĞŸĞ¾ÑĞ»Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼ HTTP server

```go
// ================================================================================
// TN-152: SIGHUP Signal Handler for Hot Reload
// ================================================================================

// setupSignalHandlers sets up signal handlers for graceful shutdown and hot reload
func setupSignalHandlers(
	cfg *appconfig.Config,
	configPath string,
	reloadCoordinator *appconfig.ReloadCoordinator,
	server *http.Server,
	timerManager *grouping.TimerManager,
) {
	// Channel for shutdown signals (SIGINT, SIGTERM)
	shutdownSignals := make(chan os.Signal, 1)
	signal.Notify(shutdownSignals, os.Interrupt, syscall.SIGTERM)

	// Channel for reload signals (SIGHUP)
	reloadSignals := make(chan os.Signal, 1)
	signal.Notify(reloadSignals, syscall.SIGHUP)

	// Goroutine for handling signals
	go func() {
		for {
			select {
			case sig := <-shutdownSignals:
				slog.Info("shutdown signal received",
					"signal", sig.String(),
				)
				handleGracefulShutdown(server, timerManager, cfg)
				return

			case sig := <-reloadSignals:
				slog.Info("reload signal received",
					"signal", sig.String(),
					"config_path", configPath,
				)
				handleConfigReload(configPath, reloadCoordinator)
			}
		}
	}()

	slog.Info("signal handlers registered",
		"shutdown_signals", []string{"SIGINT", "SIGTERM"},
		"reload_signals", []string{"SIGHUP"},
	)
}

// handleConfigReload handles SIGHUP signal for configuration reload
func handleConfigReload(
	configPath string,
	coordinator *appconfig.ReloadCoordinator,
) {
	startTime := time.Now()
	ctx := context.Background()

	slog.Info("config reload triggered",
		"trigger", "SIGHUP",
		"config_path", configPath,
	)

	// Trigger reload through coordinator
	result, err := coordinator.ReloadFromFile(ctx, configPath)
	duration := time.Since(startTime)

	if err != nil {
		slog.Error("config reload failed",
			"error", err,
			"duration_ms", duration.Milliseconds(),
		)
		// Increment error metric
		metrics.ConfigReloadTotal.WithLabelValues("error").Inc()
		metrics.ConfigReloadErrors.WithLabelValues("reload_failed").Inc()
		return
	}

	// Log success
	slog.Info("config reload successful",
		"version", result.Version,
		"components_reloaded", len(result.ComponentsReloaded),
		"duration_ms", duration.Milliseconds(),
		"rollback_occurred", result.RolledBack,
	)

	// Update metrics
	metrics.ConfigReloadTotal.WithLabelValues("success").Inc()
	metrics.ConfigReloadDuration.Observe(duration.Seconds())
	metrics.ConfigReloadLastSuccess.SetToCurrentTime()

	// Log component-specific results
	for _, comp := range result.ComponentsReloaded {
		slog.Info("component reloaded",
			"component", comp.Name,
			"duration_ms", comp.Duration.Milliseconds(),
			"success", comp.Error == nil,
		)
	}
}

// handleGracefulShutdown handles graceful shutdown (existing code)
func handleGracefulShutdown(
	server *http.Server,
	timerManager *grouping.TimerManager,
	cfg *appconfig.Config,
) {
	slog.Info("shutting down server...")

	shutdownTimeout := cfg.Server.GracefulShutdownTimeout
	if shutdownTimeout <= 0 {
		shutdownTimeout = 30 * time.Second
	}
	ctx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)
	defer cancel()

	// Shutdown timer manager first
	if timerManager != nil {
		slog.Info("shutting down timer manager...")
		if err := timerManager.Shutdown(ctx); err != nil {
			slog.Error("timer manager shutdown error", "error", err)
		} else {
			slog.Info("âœ… timer manager stopped")
		}
	}

	// Shutdown HTTP server
	if err := server.Shutdown(ctx); err != nil {
		slog.Error("server forced to shutdown", "error", err)
		os.Exit(1)
	}

	slog.Info("server exited")
	os.Exit(0)
}
```

**Integration in main()**:
```go
func main() {
	// ... existing initialization code ...

	// TN-152: Setup signal handlers (replaces old signal handling)
	setupSignalHandlers(
		cfg,
		resolvedConfigPath,
		reloadCoordinator,
		server,
		timerManager,
	)

	// Start server in goroutine
	go func() {
		slog.Info("HTTP server starting", "addr", addr)
		if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
			slog.Error("HTTP server failed to start", "error", err)
			os.Exit(1)
		}
	}()

	// Block forever (signal handlers will handle shutdown/reload)
	select {}
}
```

---

### 2. Reload Coordinator

**File**: `go-app/internal/config/reload_coordinator.go`

```go
package config

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"fmt"
	"log/slog"
	"os"
	"sync"
	"time"
)

// ================================================================================
// Configuration Reload Coordinator (TN-152)
// ================================================================================
// Orchestrates the entire hot reload process from SIGHUP signal to component reload.
//
// Features:
// - 6-phase reload pipeline (load, validate, diff, apply, reload, health check)
// - Automatic rollback on critical failures
// - Distributed locking to prevent concurrent reloads
// - Comprehensive error handling and logging
// - Prometheus metrics integration
//
// Performance Target: < 500ms p95 for typical reload
// Quality Target: 150% (Grade A+ EXCEPTIONAL)
// Author: AI Assistant
// Date: 2025-11-22

// ReloadCoordinator orchestrates configuration hot reload
type ReloadCoordinator struct {
	// Current configuration (atomic pointer for safe concurrent access)
	currentConfig atomic.Value // *Config

	// Config file path
	configPath string

	// Dependencies
	validator    *ConfigValidator
	comparator   *ConfigComparator
	reloader     *DefaultConfigReloader
	storage      ConfigStorage
	lockManager  LockManager

	// State
	mu               sync.RWMutex
	lastReloadTime   time.Time
	lastReloadStatus string
	reloadVersion    int64

	// Logger
	logger *slog.Logger
}

// NewReloadCoordinator creates a new ReloadCoordinator
func NewReloadCoordinator(
	initialConfig *Config,
	configPath string,
	validator *ConfigValidator,
	comparator *ConfigComparator,
	reloader *DefaultConfigReloader,
	storage ConfigStorage,
	lockManager LockManager,
	logger *slog.Logger,
) *ReloadCoordinator {
	if logger == nil {
		logger = slog.Default()
	}

	coordinator := &ReloadCoordinator{
		configPath:       configPath,
		validator:        validator,
		comparator:       comparator,
		reloader:         reloader,
		storage:          storage,
		lockManager:      lockManager,
		lastReloadTime:   time.Now(),
		lastReloadStatus: "initial",
		reloadVersion:    1,
		logger:           logger,
	}

	coordinator.currentConfig.Store(initialConfig)

	return coordinator
}

// ReloadResult represents the result of a reload operation
type ReloadResult struct {
	Version            int64
	Success            bool
	RolledBack         bool
	ComponentsReloaded []ComponentReloadResult
	Duration           time.Duration
	Error              error
}

// ComponentReloadResult represents reload result for a single component
type ComponentReloadResult struct {
	Name     string
	Success  bool
	Duration time.Duration
	Error    error
}

// ReloadFromFile reloads configuration from file (triggered by SIGHUP)
//
// This is the main entry point for hot reload. It orchestrates the entire
// 6-phase reload pipeline with proper error handling and rollback.
//
// Performance: < 500ms p95 for typical config
func (rc *ReloadCoordinator) ReloadFromFile(ctx context.Context, configPath string) (*ReloadResult, error) {
	startTime := time.Now()

	rc.logger.Info("reload from file started",
		"config_path", configPath,
		"current_version", rc.reloadVersion,
	)

	// Phase 1: LOAD & PARSE
	newConfig, err := rc.loadAndParse(configPath)
	if err != nil {
		rc.updateReloadStatus("load_failed")
		return nil, fmt.Errorf("phase 1 (load) failed: %w", err)
	}

	// Phase 2: VALIDATION
	validationErrors := rc.validator.ValidateAll(newConfig)
	if len(validationErrors) > 0 {
		rc.updateReloadStatus("validation_failed")
		rc.logValidationErrors(validationErrors)
		return nil, fmt.Errorf("phase 2 (validation) failed: %d error(s)", len(validationErrors))
	}

	// Phase 3: DIFF CALCULATION
	oldConfig := rc.GetCurrentConfig()
	diff := rc.comparator.Compare(oldConfig, newConfig)
	affectedComponents := rc.identifyAffectedComponents(diff)

	rc.logger.Info("config diff calculated",
		"added_fields", len(diff.Added),
		"modified_fields", len(diff.Modified),
		"deleted_fields", len(diff.Deleted),
		"affected_components", affectedComponents,
	)

	// If no changes, skip reload
	if len(diff.Modified) == 0 && len(diff.Added) == 0 && len(diff.Deleted) == 0 {
		rc.logger.Info("no config changes detected, skipping reload")
		return &ReloadResult{
			Version:  rc.reloadVersion,
			Success:  true,
			Duration: time.Since(startTime),
		}, nil
	}

	// Phase 4: ATOMIC APPLY
	if err := rc.atomicApply(ctx, oldConfig, newConfig, diff); err != nil {
		rc.updateReloadStatus("apply_failed")
		return nil, fmt.Errorf("phase 4 (apply) failed: %w", err)
	}

	// Phase 5: COMPONENT RELOAD
	componentResults := rc.reloadComponents(ctx, newConfig, affectedComponents)

	// Check for critical failures
	criticalFailed := false
	for _, result := range componentResults {
		if !result.Success && rc.isComponentCritical(result.Name) {
			criticalFailed = true
			rc.logger.Error("critical component reload failed",
				"component", result.Name,
				"error", result.Error,
			)
		}
	}

	// Rollback if critical component failed
	if criticalFailed {
		rc.logger.Warn("rolling back due to critical component failure")
		if rollbackErr := rc.rollback(ctx, oldConfig); rollbackErr != nil {
			rc.logger.Error("rollback failed", "error", rollbackErr)
			rc.updateReloadStatus("rollback_failed")
			return nil, fmt.Errorf("reload failed and rollback failed: %w", rollbackErr)
		}
		rc.updateReloadStatus("rolled_back")
		return &ReloadResult{
			Version:            rc.reloadVersion - 1,
			Success:            false,
			RolledBack:         true,
			ComponentsReloaded: componentResults,
			Duration:           time.Since(startTime),
			Error:              fmt.Errorf("critical component reload failed"),
		}, nil
	}

	// Phase 6: HEALTH CHECK
	if err := rc.healthCheck(ctx); err != nil {
		rc.logger.Warn("health check failed after reload, rolling back", "error", err)
		if rollbackErr := rc.rollback(ctx, oldConfig); rollbackErr != nil {
			rc.logger.Error("rollback failed", "error", rollbackErr)
			rc.updateReloadStatus("rollback_failed")
			return nil, fmt.Errorf("health check failed and rollback failed: %w", rollbackErr)
		}
		rc.updateReloadStatus("rolled_back")
		return &ReloadResult{
			Version:            rc.reloadVersion - 1,
			Success:            false,
			RolledBack:         true,
			ComponentsReloaded: componentResults,
			Duration:           time.Since(startTime),
			Error:              fmt.Errorf("health check failed"),
		}, nil
	}

	// SUCCESS
	rc.updateReloadStatus("success")
	duration := time.Since(startTime)

	rc.logger.Info("reload completed successfully",
		"version", rc.reloadVersion,
		"duration_ms", duration.Milliseconds(),
		"components_reloaded", len(componentResults),
	)

	return &ReloadResult{
		Version:            rc.reloadVersion,
		Success:            true,
		RolledBack:         false,
		ComponentsReloaded: componentResults,
		Duration:           duration,
	}, nil
}

// loadAndParse loads and parses configuration from file
func (rc *ReloadCoordinator) loadAndParse(configPath string) (*Config, error) {
	startTime := time.Now()

	// Read file
	data, err := os.ReadFile(configPath)
	if err != nil {
		return nil, fmt.Errorf("failed to read config file: %w", err)
	}

	// Parse using existing LoadConfig logic
	cfg, err := LoadConfig(configPath)
	if err != nil {
		return nil, fmt.Errorf("failed to parse config: %w", err)
	}

	rc.logger.Info("config loaded and parsed",
		"duration_ms", time.Since(startTime).Milliseconds(),
		"size_bytes", len(data),
		"hash", rc.calculateHash(data),
	)

	return cfg, nil
}

// atomicApply applies new configuration atomically
func (rc *ReloadCoordinator) atomicApply(
	ctx context.Context,
	oldConfig *Config,
	newConfig *Config,
	diff *ConfigDiff,
) error {
	// Acquire distributed lock
	lockKey := "config:reload"
	lock, err := rc.lockManager.Acquire(ctx, lockKey, 30*time.Second)
	if err != nil {
		return fmt.Errorf("failed to acquire lock: %w", err)
	}
	defer lock.Release(ctx)

	// Backup old config to storage (if available)
	if rc.storage != nil {
		if err := rc.storage.Backup(ctx, oldConfig); err != nil {
			rc.logger.Warn("failed to backup old config", "error", err)
			// Non-critical, continue
		}
	}

	// Atomic swap (pointer replacement)
	rc.currentConfig.Store(newConfig)

	// Increment version
	rc.mu.Lock()
	rc.reloadVersion++
	newVersion := rc.reloadVersion
	rc.mu.Unlock()

	// Write audit log (if storage available)
	if rc.storage != nil {
		rc.writeAuditLog(ctx, newVersion, diff, "sighup")
	}

	rc.logger.Info("config applied atomically",
		"new_version", newVersion,
	)

	return nil
}

// reloadComponents reloads all affected components
func (rc *ReloadCoordinator) reloadComponents(
	ctx context.Context,
	newConfig *Config,
	affectedComponents []string,
) []ComponentReloadResult {
	startTime := time.Now()

	rc.logger.Info("reloading components",
		"affected", affectedComponents,
	)

	// Call reloader (parallel execution)
	reloadErrors := rc.reloader.ReloadAll(ctx, newConfig, affectedComponents)

	// Convert to ComponentReloadResult
	results := make([]ComponentReloadResult, 0, len(affectedComponents))
	for _, compName := range affectedComponents {
		result := ComponentReloadResult{
			Name:    compName,
			Success: true,
		}

		// Find error for this component
		for _, reloadErr := range reloadErrors {
			if reloadErr.Component == compName {
				result.Success = false
				result.Error = reloadErr.Err
				result.Duration = reloadErr.Duration
				break
			}
		}

		results = append(results, result)
	}

	rc.logger.Info("components reload completed",
		"total_duration_ms", time.Since(startTime).Milliseconds(),
		"success_count", rc.countSuccessful(results),
		"failure_count", len(results)-rc.countSuccessful(results),
	)

	return results
}

// rollback rolls back to old configuration
func (rc *ReloadCoordinator) rollback(ctx context.Context, oldConfig *Config) error {
	rc.logger.Warn("initiating rollback to previous configuration")

	// Atomic swap back to old config
	rc.currentConfig.Store(oldConfig)

	// Decrement version
	rc.mu.Lock()
	rc.reloadVersion--
	rc.mu.Unlock()

	// Reload all components with old config
	reloadErrors := rc.reloader.ReloadAll(ctx, oldConfig, nil)
	if len(reloadErrors) > 0 {
		return fmt.Errorf("rollback reload failed: %d component(s) failed", len(reloadErrors))
	}

	rc.logger.Info("rollback successful")
	return nil
}

// healthCheck performs health check after reload
func (rc *ReloadCoordinator) healthCheck(ctx context.Context) error {
	// TODO: Implement comprehensive health check
	// - Check database connectivity
	// - Check Redis connectivity
	// - Verify routing engine operational
	// - Check critical services

	rc.logger.Info("health check passed")
	return nil
}

// GetCurrentConfig returns current configuration (thread-safe)
func (rc *ReloadCoordinator) GetCurrentConfig() *Config {
	return rc.currentConfig.Load().(*Config)
}

// GetReloadStatus returns current reload status
func (rc *ReloadCoordinator) GetReloadStatus() (version int64, status string, lastReload time.Time) {
	rc.mu.RLock()
	defer rc.mu.RUnlock()
	return rc.reloadVersion, rc.lastReloadStatus, rc.lastReloadTime
}

// updateReloadStatus updates reload status
func (rc *ReloadCoordinator) updateReloadStatus(status string) {
	rc.mu.Lock()
	defer rc.mu.Unlock()
	rc.lastReloadStatus = status
	rc.lastReloadTime = time.Now()
}

// identifyAffectedComponents identifies components affected by config changes
func (rc *ReloadCoordinator) identifyAffectedComponents(diff *ConfigDiff) []string {
	affected := make(map[string]bool)

	// Check which sections changed
	for field := range diff.Modified {
		if startsWith(field, "route") || startsWith(field, "routes") {
			affected["routing"] = true
		}
		if startsWith(field, "receivers") {
			affected["receivers"] = true
		}
		if startsWith(field, "inhibit_rules") {
			affected["inhibition"] = true
		}
		if startsWith(field, "silences") {
			affected["silencing"] = true
		}
		if startsWith(field, "grouping") {
			affected["grouping"] = true
		}
		if startsWith(field, "llm") {
			affected["llm"] = true
		}
		if startsWith(field, "database") {
			affected["database"] = true
		}
		if startsWith(field, "redis") {
			affected["redis"] = true
		}
	}

	// Convert to slice
	components := make([]string, 0, len(affected))
	for comp := range affected {
		components = append(components, comp)
	}

	return components
}

// isComponentCritical checks if component is critical
func (rc *ReloadCoordinator) isComponentCritical(componentName string) bool {
	// Critical components that must succeed for reload to be considered successful
	criticalComponents := map[string]bool{
		"routing":   true,
		"receivers": true,
		"database":  true,
		"grouping":  true,
	}
	return criticalComponents[componentName]
}

// calculateHash calculates SHA256 hash of data
func (rc *ReloadCoordinator) calculateHash(data []byte) string {
	hash := sha256.Sum256(data)
	return hex.EncodeToString(hash[:])
}

// countSuccessful counts successful component reloads
func (rc *ReloadCoordinator) countSuccessful(results []ComponentReloadResult) int {
	count := 0
	for _, r := range results {
		if r.Success {
			count++
		}
	}
	return count
}

// logValidationErrors logs validation errors
func (rc *ReloadCoordinator) logValidationErrors(errors []ValidationErrorDetail) {
	for _, err := range errors {
		rc.logger.Error("validation error",
			"field", err.Field,
			"message", err.Message,
			"code", err.Code,
		)
	}
}

// writeAuditLog writes audit log entry
func (rc *ReloadCoordinator) writeAuditLog(
	ctx context.Context,
	version int64,
	diff *ConfigDiff,
	source string,
) {
	// TODO: Write to PostgreSQL audit log table
	rc.logger.Info("audit log",
		"version", version,
		"source", source,
		"added_fields", len(diff.Added),
		"modified_fields", len(diff.Modified),
		"deleted_fields", len(diff.Deleted),
	)
}

// Helper function
func startsWith(s, prefix string) bool {
	return len(s) >= len(prefix) && s[:len(prefix)] == prefix
}
```

---

### 3. Prometheus Metrics

**File**: `go-app/internal/metrics/config_reload.go`

```go
package metrics

import (
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)

// ================================================================================
// TN-152: Config Reload Metrics
// ================================================================================

var (
	// ConfigReloadTotal tracks total reload attempts by status
	ConfigReloadTotal = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "config_reload_total",
			Help: "Total number of config reload attempts",
		},
		[]string{"status"}, // success, error, validation_failed, rolled_back
	)

	// ConfigReloadDuration tracks reload duration histogram
	ConfigReloadDuration = promauto.NewHistogram(
		prometheus.HistogramOpts{
			Name:    "config_reload_duration_seconds",
			Help:    "Duration of config reload operations",
			Buckets: []float64{0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0},
		},
	)

	// ConfigReloadPhaseDuration tracks duration by phase
	ConfigReloadPhaseDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "config_reload_phase_duration_seconds",
			Help:    "Duration of config reload phases",
			Buckets: []float64{0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5},
		},
		[]string{"phase"}, // load, validate, diff, apply, reload, health_check
	)

	// ConfigReloadComponentDuration tracks component reload duration
	ConfigReloadComponentDuration = promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "config_reload_component_duration_seconds",
			Help:    "Duration of component reload operations",
			Buckets: []float64{0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0},
		},
		[]string{"component"}, // routing, receivers, inhibition, etc.
	)

	// ConfigReloadErrors tracks reload errors by type
	ConfigReloadErrors = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "config_reload_errors_total",
			Help: "Total number of config reload errors by type",
		},
		[]string{"type"}, // load_failed, validation_failed, apply_failed, etc.
	)

	// ConfigReloadLastSuccess tracks last successful reload timestamp
	ConfigReloadLastSuccess = promauto.NewGauge(
		prometheus.GaugeOpts{
			Name: "config_reload_last_success_timestamp_seconds",
			Help: "Timestamp of last successful config reload",
		},
	)

	// ConfigReloadRollbacks tracks rollback count
	ConfigReloadRollbacks = promauto.NewCounterVec(
		prometheus.CounterOpts{
			Name: "config_reload_rollbacks_total",
			Help: "Total number of config reload rollbacks",
		},
		[]string{"reason"}, // critical_failed, timeout, health_check_failed
	)

	// ConfigReloadVersion tracks current config version
	ConfigReloadVersion = promauto.NewGauge(
		prometheus.GaugeOpts{
			Name: "config_reload_version",
			Help: "Current configuration version",
		},
	)
)
```

---

### 4. Health Check Endpoint

**File**: `go-app/cmd/server/handlers/config_status.go`

```go
package handlers

import (
	"encoding/json"
	"net/http"
	"time"

	appconfig "github.com/vitaliisemenov/alert-history/internal/config"
)

// ConfigStatusHandler handles GET /api/v2/config/status
type ConfigStatusHandler struct {
	coordinator *appconfig.ReloadCoordinator
}

// NewConfigStatusHandler creates a new handler
func NewConfigStatusHandler(coordinator *appconfig.ReloadCoordinator) *ConfigStatusHandler {
	return &ConfigStatusHandler{
		coordinator: coordinator,
	}
}

// HandleGetStatus handles GET /api/v2/config/status
func (h *ConfigStatusHandler) HandleGetStatus(w http.ResponseWriter, r *http.Request) {
	version, status, lastReload := h.coordinator.GetReloadStatus()

	response := ConfigStatusResponse{
		Version:        version,
		Status:         status,
		LastReload:     lastReload.Format(time.RFC3339),
		LastReloadUnix: lastReload.Unix(),
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusOK)
	json.NewEncoder(w).Encode(response)
}

// ConfigStatusResponse represents status response
type ConfigStatusResponse struct {
	Version        int64  `json:"version"`
	Status         string `json:"status"`
	LastReload     string `json:"last_reload"`
	LastReloadUnix int64  `json:"last_reload_unix"`
}
```

---

## ğŸ§ª Testing Strategy

### 1. Unit Tests (â‰¥25 tests, 90% coverage)

**File**: `go-app/internal/config/reload_coordinator_test.go`

```go
package config_test

import (
	"context"
	"os"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestReloadCoordinator_ReloadFromFile_Success(t *testing.T) {
	// Test successful reload
}

func TestReloadCoordinator_ReloadFromFile_ValidationError(t *testing.T) {
	// Test validation error handling
}

func TestReloadCoordinator_ReloadFromFile_ComponentFailure(t *testing.T) {
	// Test component reload failure
}

func TestReloadCoordinator_ReloadFromFile_Rollback(t *testing.T) {
	// Test rollback mechanism
}

func TestReloadCoordinator_ReloadFromFile_NoChanges(t *testing.T) {
	// Test no-op when config unchanged
}

func TestReloadCoordinator_ReloadFromFile_ConcurrentReload(t *testing.T) {
	// Test concurrent reload prevention
}

// ... 19 more tests
```

### 2. Integration Tests (â‰¥10 tests)

**File**: `go-app/internal/config/reload_integration_test.go`

```go
func TestSIGHUP_EndToEnd(t *testing.T) {
	// Test end-to-end SIGHUP handling
	// 1. Start server
	// 2. Send SIGHUP
	// 3. Verify config reloaded
	// 4. Verify components reloaded
	// 5. Verify metrics updated
}

func TestSIGHUP_WithKubernetes(t *testing.T) {
	// Test Kubernetes ConfigMap update scenario
}

// ... 8 more integration tests
```

### 3. Benchmarks (â‰¥5)

```go
func BenchmarkReloadFromFile_Small(b *testing.B) {
	// Benchmark small config reload (< 100 LOC)
}

func BenchmarkReloadFromFile_Large(b *testing.B) {
	// Benchmark large config reload (> 1000 LOC)
}

func BenchmarkComponentReload_Parallel(b *testing.B) {
	// Benchmark parallel component reload
}

// ... 2 more benchmarks
```

---

## ğŸ“Š Performance Targets

| Operation | Target (150%) | Baseline (100%) |
|-----------|---------------|-----------------|
| Total Reload | < 300ms p95 | < 500ms p95 |
| Phase 1 (Load) | < 30ms | < 50ms |
| Phase 2 (Validate) | < 60ms | < 100ms |
| Phase 3 (Diff) | < 15ms | < 20ms |
| Phase 4 (Apply) | < 30ms | < 50ms |
| Phase 5 (Reload) | < 180ms | < 300ms |
| Phase 6 (Health) | < 30ms | < 50ms |
| Rollback | < 150ms | < 200ms |

---

## ğŸ“ Implementation Checklist

### Phase 1: Core Infrastructure (2-3h)
- [ ] Create reload_coordinator.go
- [ ] Implement ReloadCoordinator struct
- [ ] Implement ReloadFromFile method
- [ ] Add 6-phase pipeline
- [ ] Add rollback mechanism

### Phase 2: Signal Handling (1-2h)
- [ ] Update main.go with setupSignalHandlers
- [ ] Register SIGHUP handler
- [ ] Integrate with ReloadCoordinator
- [ ] Add graceful shutdown handling

### Phase 3: Metrics & Observability (1h)
- [ ] Add Prometheus metrics
- [ ] Add structured logging
- [ ] Create status endpoint
- [ ] Add health check

### Phase 4: Testing (2-3h)
- [ ] Unit tests (â‰¥25)
- [ ] Integration tests (â‰¥10)
- [ ] Benchmarks (â‰¥5)
- [ ] End-to-end tests

### Phase 5: Documentation (1-2h)
- [ ] User guide
- [ ] Kubernetes integration guide
- [ ] Troubleshooting guide
- [ ] API documentation

---

**Document Version**: 1.0
**Last Updated**: 2025-11-22
**Author**: AI Assistant
**Total Lines**: 1,200+ LOC
**Status**: âœ… Ready for Implementation
